{"train/reward_0": 0.0, "train/reward_1": 0.00011141444215848836, "train/reward_2": -1.0, "train/grid2opsteps": 1308, "_timestamp": 1722409572.4239395, "_runtime": 103.03409147262573, "_step": 64, "losses_1/value_loss": 0.6481327414512634, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0051244767382740974, "losses_1/entropy": 3.9698777198791504, "losses_1/old_approx_kl": -0.00029155611991882324, "losses_1/approx_kl": 2.1457672119140625e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.04050570726394653, "global_step": 48, "_wandb": {"runtime": 105}}