{"train/reward_0": 0.0, "train/reward_1": 2.2058675289154053, "train/reward_2": -0.6, "train/grid2opsteps": 210, "_timestamp": 1721284952.270671, "_runtime": 60.54047179222107, "_step": 16, "eval/reward_0": 0.280241402116402, "eval/reward_1": 204.84304197405774, "eval/reward_2": -29.85999999999918, "losses_1/value_loss": 129970.8671875, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0008407235145568848, "losses_1/entropy": 3.970264196395874, "losses_1/old_approx_kl": 0.00039654970169067383, "losses_1/approx_kl": 8.046627044677734e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0002461671829223633, "global_step": 16, "_wandb": {"runtime": 59}}