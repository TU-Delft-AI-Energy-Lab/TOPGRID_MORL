{"train/reward_ScaledLinesCapacity": 0.9347773836254488, "train/reward_TopoDepth": 863.1428571428769, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 2016, "_timestamp": 1722521354.2237413, "_runtime": 1092.765189409256, "_step": 1024, "eval/reward_ScaledLinesCapacity": 0.379627309234077, "eval/reward_TopoDepth": 218.57142857143745, "eval/reward_TopoAction": -1.0, "eval/steps": 2016.0, "losses_1/value_loss": 12139.3173828125, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.029380004853010178, "losses_1/entropy": 3.5422191619873047, "losses_1/old_approx_kl": 0.0073893144726753235, "losses_1/approx_kl": 0.010676087811589241, "losses_1/clipfrac": 0.0375, "losses_1/explained_variance": 0.025183439254760742, "global_step": 1024, "_wandb": {"runtime": 1092}}