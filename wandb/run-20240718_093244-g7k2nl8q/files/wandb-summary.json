{"train/reward_0": 0.255332350730896, "train/reward_1": -0.7625629028400556, "train/reward_2": -0.1, "train/grid2opsteps": 231, "_timestamp": 1721288085.5990489, "_runtime": 121.2320909500122, "_step": 32, "eval/reward_0": 79.47754509720222, "eval/reward_1": -206.64406402984926, "eval/reward_2": -31.20666666666589, "losses_1/value_loss": 113578.1796875, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 0.0003541409969329834, "losses_1/entropy": 3.970268487930298, "losses_1/old_approx_kl": 0.0001456141471862793, "losses_1/approx_kl": 1.341104507446289e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.00014030933380126953, "global_step": 32, "_wandb": {"runtime": 119}}