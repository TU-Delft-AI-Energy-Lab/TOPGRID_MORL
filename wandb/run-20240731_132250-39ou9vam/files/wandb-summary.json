{"train/reward_0": 0.0, "train/reward_1": 0.04094345324451381, "train/reward_2": 0.0, "train/grid2opsteps": 94, "_timestamp": 1722425098.5837555, "_runtime": 128.47323751449585, "_step": 256, "eval/reward_0": 0.2575024801587302, "eval/reward_1": 0.24435603973540113, "eval/reward_2": -0.5, "eval/steps": 1068.5, "losses_1/value_loss": 0.7183286547660828, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15013998746871948, "losses_1/entropy": 3.785787582397461, "losses_1/old_approx_kl": -0.016109585762023926, "losses_1/approx_kl": 0.2333889901638031, "losses_1/clipfrac": 0.6625, "losses_1/explained_variance": -0.395829439163208, "global_step": 256, "_wandb": {"runtime": 128}}