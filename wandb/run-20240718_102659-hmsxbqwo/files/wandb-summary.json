{"train/reward_0": 0.0, "train/reward_1": 0.00029439967796528296, "train/reward_2": -40.0, "train/grid2opsteps": 333, "_timestamp": 1721291269.346285, "_runtime": 49.36666011810303, "_step": 24, "eval/reward_0": 0.20785383597883597, "eval/reward_1": 0.10987905177272887, "eval/reward_2": -22.566666666666666, "losses_1/value_loss": 729.2503051757812, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": 0.0005016922950744629, "losses_1/entropy": 3.9702701568603516, "losses_1/old_approx_kl": -0.0009851455688476562, "losses_1/approx_kl": 1.773238182067871e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.00283735990524292, "global_step": 16, "_wandb": {"runtime": 54}}