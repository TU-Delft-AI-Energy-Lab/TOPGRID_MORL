{"train/reward_0": 0.0, "train/reward_1": 0.0002556701469038331, "train/reward_2": 0.0, "train/grid2opsteps": 769, "_timestamp": 1721370377.5147462, "_runtime": 82.86461114883423, "_step": 16, "eval/reward_0": 0.16765873015873015, "eval/reward_1": 0.14389642882288753, "eval/reward_2": -0.13333333333333333, "losses_1/value_loss": 0.3556138575077057, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.00019875168800354004, "losses_1/entropy": 3.9702658653259277, "losses_1/old_approx_kl": 0.0008851885795593262, "losses_1/approx_kl": 1.5795230865478516e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.7374885082244873, "global_step": 16, "_wandb": {"runtime": 81}}