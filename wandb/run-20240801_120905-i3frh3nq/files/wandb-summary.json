{"train/reward_ScaledLinesCapacity": 0.00021938815838894364, "train/reward_ScaledDistance": 0.00042328042328042324, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 88, "_timestamp": 1722506949.2057676, "_runtime": 4.02996563911438, "_step": 41, "eval/reward_ScaledLinesCapacity": 6.0392310477070716e-05, "eval/reward_ScaledDistance": 0.00014109347442680775, "eval/reward_TopoAction": -1.0, "eval/steps": 2.0, "losses_1/value_loss": 13.97234058380127, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.08550038188695908, "losses_1/entropy": 3.909864664077759, "losses_1/old_approx_kl": -0.2790757119655609, "losses_1/approx_kl": 0.09696602076292038, "losses_1/clipfrac": 0.45555556043982504, "losses_1/explained_variance": -4.481490135192871, "global_step": 36, "_wandb": {"runtime": 4}}