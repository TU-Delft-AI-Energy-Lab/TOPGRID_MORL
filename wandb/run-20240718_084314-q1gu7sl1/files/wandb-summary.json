{"train/reward_0": 0.0, "train/reward_1": 221.65638272464275, "train/reward_2": -37.000000000000256, "train/grid2opsteps": 372, "_timestamp": 1721285054.9121022, "_runtime": 60.5635552406311, "_step": 16, "eval/reward_0": 0.21579034391534388, "eval/reward_1": 162.14514816254376, "eval/reward_2": -24.81333333333283, "losses_1/value_loss": 1881.81689453125, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.00032845139503479004, "losses_1/entropy": 3.970273494720459, "losses_1/old_approx_kl": -0.0006868839263916016, "losses_1/approx_kl": 1.0728836059570312e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0011535882949829102, "global_step": 16, "_wandb": {"runtime": 59}}