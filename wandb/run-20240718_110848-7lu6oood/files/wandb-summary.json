{"train/reward_0": 0.0, "train/reward_1": 0.00022447319099500423, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1721293847.5024436, "_runtime": 119.16731452941895, "_step": 32, "eval/reward_0": 0.05543154761904761, "eval/reward_1": 0.07155803346799368, "eval/reward_2": -0.2, "losses_1/value_loss": 0.2787887752056122, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.00029368698596954346, "losses_1/entropy": 3.9702489376068115, "losses_1/old_approx_kl": -1.9788742065429688e-05, "losses_1/approx_kl": 1.6391277313232422e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.30865204334259033, "global_step": 32, "_wandb": {"runtime": 118}}