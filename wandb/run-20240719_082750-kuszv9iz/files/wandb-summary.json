{"train/reward_0": 0.0, "train/reward_1": 7.510597240053515e-05, "train/reward_2": 0.0, "train/grid2opsteps": 2600, "_timestamp": 1721370533.8082006, "_runtime": 63.549349546432495, "_step": 16, "eval/reward_0": 0.17584325396825395, "eval/reward_1": 0.17215407561865625, "eval/reward_2": -0.16666666666666666, "losses_1/value_loss": 0.1568146049976349, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -9.033083915710449e-05, "losses_1/entropy": 3.9702694416046143, "losses_1/old_approx_kl": 0.0004602670669555664, "losses_1/approx_kl": 1.1324882507324219e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.3884497880935669, "global_step": 16, "_wandb": {"runtime": 62}}