{"train/reward_ScaledLinesCapacity": 0.1317428302142505, "train/reward_ScaledDistance": 0.16955026455026462, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 377, "_timestamp": 1722503508.2397614, "_runtime": 51.946770429611206, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.003631431301788737, "eval/reward_ScaledDistance": 0.007901234567901221, "eval/reward_TopoAction": -0.8333333333333333, "eval/steps": 44.5, "losses_1/value_loss": 8.16291618347168, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.095152348279953, "losses_1/entropy": 3.8559277057647705, "losses_1/old_approx_kl": -0.13280756771564484, "losses_1/approx_kl": 0.028435949236154556, "losses_1/clipfrac": 0.18888888992369174, "losses_1/explained_variance": -12.28253173828125, "global_step": 108, "_wandb": {"runtime": 51}}