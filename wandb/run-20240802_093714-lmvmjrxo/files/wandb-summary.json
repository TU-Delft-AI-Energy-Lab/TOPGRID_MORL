{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_TopoDepth": 0.0, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722584420.9862587, "_runtime": 186.01215267181396, "_step": 384, "eval/reward_ScaledLinesCapacity": 0.0035973288887178457, "eval/reward_TopoDepth": -1.9557823129251692, "eval/reward_TopoAction": -1.0, "eval/steps": 73.5, "losses_1/value_loss": 11461.9033203125, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.014692634344100952, "losses_1/entropy": 3.754774570465088, "losses_1/old_approx_kl": -0.022009149193763733, "losses_1/approx_kl": 0.02934342436492443, "losses_1/clipfrac": 0.15, "losses_1/explained_variance": -0.002504110336303711, "global_step": 320, "_wandb": {"runtime": 188}}