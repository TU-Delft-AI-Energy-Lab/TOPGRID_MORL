{"train/reward_0": 1.0, "train/reward_1": 0.6385741113496936, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722429028.3906615, "_runtime": 112.1586365699768, "_step": 280, "eval/reward_0": 0.25012400793650796, "eval/reward_1": 0.23232105789494728, "eval/reward_2": -0.75, "eval/steps": 1008.5, "losses_1/value_loss": 4.03054666519165, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.09863495081663132, "losses_1/entropy": 3.897296190261841, "losses_1/old_approx_kl": 0.05215032398700714, "losses_1/approx_kl": 0.04631036892533302, "losses_1/clipfrac": 0.2265625, "losses_1/explained_variance": -3.1538243293762207, "global_step": 256, "_wandb": {"runtime": 112}}