{"train/reward_0": 0.0, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 380, "_timestamp": 1722454767.4018614, "_runtime": 135.7913293838501, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.2388877010871505, "eval/reward_Distance": 380.0492063492097, "eval/reward_TopoAction": -0.6333333333333333, "eval/steps": 2016.0, "losses_1/value_loss": 80164.703125, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07263971865177155, "losses_1/entropy": 3.8758456707000732, "losses_1/old_approx_kl": 0.020899521186947823, "losses_1/approx_kl": 0.026688814163208008, "losses_1/clipfrac": 0.19444445110857486, "losses_1/explained_variance": -0.000811457633972168, "global_step": 108, "_wandb": {"runtime": 135}}