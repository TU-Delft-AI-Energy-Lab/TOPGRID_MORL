{"train/reward_0": 1.0, "train/reward_1": 0.8759656816538232, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721738975.581735, "_runtime": 140.78875279426575, "_step": 128, "eval/reward_0": 0.0757853835978836, "eval/reward_1": 0.06892179843163868, "eval/reward_2": -0.5722222222222223, "losses_1/value_loss": 0.40554723143577576, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": -0.00018299929797649384, "losses_1/entropy": 3.9700047969818115, "losses_1/old_approx_kl": 8.371472358703613e-05, "losses_1/approx_kl": 3.725290298461914e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.29123640060424805, "global_step": 128, "_wandb": {"runtime": 139}}