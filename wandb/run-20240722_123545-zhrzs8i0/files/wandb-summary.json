{"train/reward_0": 0.000248015873015873, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721644571.9416463, "_runtime": 25.93962335586548, "_step": 16, "eval/reward_0": 0.16674933862433863, "eval/reward_1": 0.1419241100793898, "eval/reward_2": -0.16666666666666666, "losses_1/value_loss": 0.12695811688899994, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.00035700201988220215, "losses_1/entropy": 3.9702625274658203, "losses_1/old_approx_kl": -0.00032275915145874023, "losses_1/approx_kl": 1.341104507446289e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.04461407661437988, "global_step": 16, "_wandb": {"runtime": 24}}