{"train/reward_0": 0.6596748060267914, "train/reward_1": 2.0, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722432386.1928298, "_runtime": 424.18566274642944, "_step": 911, "eval/reward_0": 0.3931068349326269, "eval/reward_1": 1.0002480158730158, "eval/reward_2": -0.5, "eval/steps": 1008.5, "losses_1/value_loss": 0.6031119227409363, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.16203059256076813, "losses_1/entropy": 3.80570387840271, "losses_1/old_approx_kl": 0.06644133478403091, "losses_1/approx_kl": 0.1244189590215683, "losses_1/clipfrac": 0.5729166666666666, "losses_1/explained_variance": 0.28213566541671753, "global_step": 768, "_wandb": {"runtime": 425}}