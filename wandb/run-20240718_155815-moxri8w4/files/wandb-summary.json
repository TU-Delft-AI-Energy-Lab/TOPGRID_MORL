{"train/reward_0": 0.0, "train/reward_1": 6.129048013510429e-05, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1721311216.2589302, "_runtime": 121.02112412452698, "_step": 32, "eval/reward_0": 0.13134093915343917, "eval/reward_1": 0.138465183483652, "eval/reward_2": -0.23333333333333334, "losses_1/value_loss": 0.2506880760192871, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 8.657574653625488e-05, "losses_1/entropy": 3.970245122909546, "losses_1/old_approx_kl": -0.00028520822525024414, "losses_1/approx_kl": 5.960464477539063e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.10012364387512207, "global_step": 32, "_wandb": {"runtime": 119}}