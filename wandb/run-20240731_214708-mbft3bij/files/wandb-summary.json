{"train/reward_ScaledLinesCapacity": 0.04503283536022712, "train/reward_Distance": 110.4761904761902, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 116, "_timestamp": 1722455375.6246533, "_runtime": 146.60130739212036, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.005323835110600082, "eval/reward_Distance": 21.011904761904763, "eval/reward_TopoAction": -0.75, "eval/steps": 43.5, "losses_1/value_loss": 165602.890625, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07530614733695984, "losses_1/entropy": 3.8272671699523926, "losses_1/old_approx_kl": 0.2414952963590622, "losses_1/approx_kl": 0.04831508919596672, "losses_1/clipfrac": 0.21111111715435982, "losses_1/explained_variance": -0.005368471145629883, "global_step": 108, "_wandb": {"runtime": 145}}