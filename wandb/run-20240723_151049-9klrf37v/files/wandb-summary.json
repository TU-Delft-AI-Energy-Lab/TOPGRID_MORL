{"train/reward_0": 1.0, "train/reward_1": 0.8145318316905898, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721740411.1667109, "_runtime": 161.17254996299744, "_step": 128, "eval/reward_0": 0.012913359788359788, "eval/reward_1": 0.028842294967726192, "eval/reward_2": -0.5944444444444446, "losses_1/value_loss": 0.5706085562705994, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": -0.00013843923807144165, "losses_1/entropy": 3.969898223876953, "losses_1/old_approx_kl": 5.415081977844238e-05, "losses_1/approx_kl": 7.450580596923828e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.9093519449234009, "global_step": 128, "_wandb": {"runtime": 159}}