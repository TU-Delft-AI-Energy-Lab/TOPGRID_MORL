{"train/reward_0": 0.0, "train/reward_1": 0.0006571441655280862, "train/reward_2": 0.0, "train/grid2opsteps": 292, "_timestamp": 1721292756.6363063, "_runtime": 30.625152349472046, "_step": 19, "eval/reward_0": 0.014872685185185187, "eval/reward_1": 0.023407313389196924, "eval/reward_2": -Infinity, "losses_1/value_loss": 61.67242431640625, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": 0.0005461201071739197, "losses_1/entropy": 3.970264434814453, "losses_1/old_approx_kl": -0.0007854104042053223, "losses_1/approx_kl": 7.599592208862305e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.006704926490783691, "global_step": 16, "_wandb": {"runtime": 31}}