{"train/reward_0": 0.0, "train/reward_1": 0.0002725986116029039, "train/reward_2": -60.0, "train/grid2opsteps": 3, "_timestamp": 1721293339.705541, "_runtime": 59.10002398490906, "_step": 24, "eval/reward_0": 0.11003637566137565, "eval/reward_1": 0.10954848796380358, "eval/reward_2": -0.23333333333333334, "losses_1/value_loss": 966.6707153320312, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -0.0011035501956939697, "losses_1/entropy": 3.970271110534668, "losses_1/old_approx_kl": -0.0008722543716430664, "losses_1/approx_kl": 2.2649765014648438e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.00025153160095214844, "global_step": 16, "_wandb": {"runtime": 70}}