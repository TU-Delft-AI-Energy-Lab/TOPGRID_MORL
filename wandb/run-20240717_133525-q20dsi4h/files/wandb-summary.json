{"train/reward_0": 0.0, "train/reward_1": 274.15820506960154, "train/reward_2": -32.800000000000196, "train/grid2opsteps": 441, "_timestamp": 1721216166.609553, "_runtime": 41.07012414932251, "_step": 16, "eval/reward_0": 0.16689814814814816, "eval/reward_1": 162.45169876640043, "eval/reward_2": -22.659999999999744, "losses_1/value_loss": 37218.375, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 1.2791715562343597e-05, "losses_1/entropy": 3.970247745513916, "losses_1/old_approx_kl": -0.00014388561248779297, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0005587339401245117, "global_step": 16, "_wandb": {"runtime": 40}}