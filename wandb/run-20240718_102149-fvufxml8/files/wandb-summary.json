{"train/reward_0": 0.0, "train/reward_1": 0.0002938402647812488, "train/reward_2": 0.0, "train/grid2opsteps": 6, "_timestamp": 1721291032.7591693, "_runtime": 123.24923539161682, "_step": 32, "eval/reward_0": 0.4699570105820106, "eval/reward_1": 0.24186346006135515, "eval/reward_2": -18.8, "losses_1/value_loss": 426.6883239746094, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 3.014504909515381e-05, "losses_1/entropy": 3.9702463150024414, "losses_1/old_approx_kl": 8.976459503173828e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.013845562934875488, "global_step": 32, "_wandb": {"runtime": 122}}