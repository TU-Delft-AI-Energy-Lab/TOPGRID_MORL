{"train/reward_0": 0.0, "train/reward_1": 0.00022594551246404762, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722414995.7299721, "_runtime": 24.1568500995636, "_step": 64, "eval/reward_0": 0.10012400793650794, "eval/reward_1": 0.08210152246566352, "eval/reward_2": -0.7833333333333333, "eval/steps": 1009.0, "losses_1/value_loss": 8.925858497619629, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15096303820610046, "losses_1/entropy": 3.9256417751312256, "losses_1/old_approx_kl": 0.13809187710285187, "losses_1/approx_kl": 0.14598314464092255, "losses_1/clipfrac": 0.5770833333333333, "losses_1/explained_variance": -6.542699337005615, "global_step": 64, "_wandb": {"runtime": 23}}