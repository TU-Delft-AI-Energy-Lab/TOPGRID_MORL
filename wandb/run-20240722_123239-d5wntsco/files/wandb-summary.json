{"train/reward_0": 0.026041666666666668, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 494, "_timestamp": 1721644380.6307924, "_runtime": 20.965441465377808, "_step": 16, "eval/reward_0": 0.1418650793650794, "eval/reward_1": 0.11563995320555999, "eval/reward_2": -0.26666666666666666, "losses_1/value_loss": 0.35895517468452454, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0004096999764442444, "losses_1/entropy": 3.970269203186035, "losses_1/old_approx_kl": 0.00043761730194091797, "losses_1/approx_kl": 2.5331974029541016e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -3.596740245819092, "global_step": 16, "_wandb": {"runtime": 20}}