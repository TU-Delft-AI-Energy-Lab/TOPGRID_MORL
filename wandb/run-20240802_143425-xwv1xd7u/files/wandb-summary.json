{"train/reward_ScaledLinesCapacity": 0.8402728057034327, "train/reward_ScaledTopoDepth": -0.31999999999999557, "train/reward_SubstationSwitching": -11.23404255319149, "train/grid2opsteps": 2016, "_timestamp": 1722602160.193854, "_runtime": 94.43778204917908, "_step": 249, "eval/reward_ScaledLinesCapacity": 8.461142139703716e-05, "eval/reward_ScaledTopoDepth": -9.259259259259259e-05, "eval/reward_SubstationSwitching": -2.0, "eval/steps": 2.0, "losses_1/value_loss": 6.9917311668396, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.09704539179801941, "losses_1/entropy": 3.859919548034668, "losses_1/old_approx_kl": 0.053327664732933044, "losses_1/approx_kl": 0.04401931166648865, "losses_1/clipfrac": 0.240625, "losses_1/explained_variance": 0.10231506824493408, "global_step": 192, "_wandb": {"runtime": 94}}