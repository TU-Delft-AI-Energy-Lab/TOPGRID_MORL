{"train/reward_0": 0.0, "train/reward_1": 0.00018940741395526584, "train/reward_2": -1.0, "train/grid2opsteps": 813, "_timestamp": 1722409400.2138414, "_runtime": 46.08161640167236, "_step": 48, "losses_1/value_loss": 2.3214499950408936, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -0.005483567714691162, "losses_1/entropy": 3.9700679779052734, "losses_1/old_approx_kl": 0.0036676228046417236, "losses_1/approx_kl": 4.809349775314331e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.05795109272003174, "global_step": 32, "_wandb": {"runtime": 66}}