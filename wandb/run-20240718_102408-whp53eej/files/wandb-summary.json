{"train/reward_0": 0.0, "train/reward_1": 0.0004979315864537444, "train/reward_2": -50.0, "train/grid2opsteps": 2186, "_timestamp": 1721291204.472504, "_runtime": 155.63551783561707, "_step": 32, "eval/reward_0": 0.22435515873015877, "eval/reward_1": 0.11996414277216863, "eval/reward_2": -24.433333333333334, "losses_1/value_loss": 1412.4720458984375, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 2.11372971534729e-05, "losses_1/entropy": 3.970245838165283, "losses_1/old_approx_kl": -0.00012552738189697266, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.004328727722167969, "global_step": 32, "_wandb": {"runtime": 154}}