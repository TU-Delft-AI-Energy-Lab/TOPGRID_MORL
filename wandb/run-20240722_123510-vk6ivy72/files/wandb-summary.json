{"train/reward_0": 0.0, "train/reward_1": 0.38459054412975757, "train/reward_2": 0.0, "train/grid2opsteps": 810, "_timestamp": 1721644535.2672544, "_runtime": 24.62024426460266, "_step": 16, "eval/reward_0": 0.03796296296296296, "eval/reward_1": 0.03667734124195789, "eval/reward_2": -0.26666666666666666, "losses_1/value_loss": 0.2410515695810318, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0007910579442977905, "losses_1/entropy": 3.970266580581665, "losses_1/old_approx_kl": 0.000731050968170166, "losses_1/approx_kl": 1.7881393432617188e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.08163279294967651, "global_step": 16, "_wandb": {"runtime": 23}}