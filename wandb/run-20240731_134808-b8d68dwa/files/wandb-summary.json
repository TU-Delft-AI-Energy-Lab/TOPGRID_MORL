{"train/reward_0": 0.02703373015873016, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 109, "_timestamp": 1722426625.39879, "_runtime": 137.14202880859375, "_step": 256, "eval/reward_0": 0.000248015873015873, "eval/reward_1": 0.00011672701350577862, "eval/reward_2": -1.0, "eval/steps": 3.0, "losses_1/value_loss": 0.6283578872680664, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15425755083560944, "losses_1/entropy": 3.8596224784851074, "losses_1/old_approx_kl": 0.05038954317569733, "losses_1/approx_kl": 0.12847091257572174, "losses_1/clipfrac": 0.56875, "losses_1/explained_variance": 0.4852333664894104, "global_step": 256, "_wandb": {"runtime": 136}}