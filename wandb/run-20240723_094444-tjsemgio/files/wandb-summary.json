{"train/reward_0": 0.0, "train/reward_1": 0.00023702432633044768, "train/reward_2": 0.0, "train/grid2opsteps": 109, "_timestamp": 1721720763.003373, "_runtime": 78.0312728881836, "_step": 70, "eval/reward_0": 1.0, "eval/reward_1": 0.6843243393746817, "eval/reward_2": 0.0, "losses_1/value_loss": 0.42135104537010193, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": 0.0008958876132965088, "losses_1/entropy": 3.970252275466919, "losses_1/old_approx_kl": 0.00012942403554916382, "losses_1/approx_kl": 1.800432801246643e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -3.3891963958740234, "global_step": 64, "_wandb": {"runtime": 76}}