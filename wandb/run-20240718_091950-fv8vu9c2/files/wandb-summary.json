{"train/reward_0": 0.0, "train/reward_1": 0.7004996836185455, "train/reward_2": -0.1, "train/grid2opsteps": 186, "_timestamp": 1721287277.6740463, "_runtime": 87.41715335845947, "_step": 32, "eval/reward_0": 0.16451719576719578, "eval/reward_1": 131.29233244160812, "eval/reward_2": -19.876666666666328, "losses_1/value_loss": 914.1751098632812, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 0.0003574788570404053, "losses_1/entropy": 3.9702353477478027, "losses_1/old_approx_kl": -0.0001493692398071289, "losses_1/approx_kl": 1.6391277313232422e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0007622241973876953, "global_step": 32, "_wandb": {"runtime": 86}}