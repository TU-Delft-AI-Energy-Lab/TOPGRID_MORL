{"train/reward_0": 0.00018863208123152082, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 121, "_timestamp": 1722433404.4210274, "_runtime": 382.905056476593, "_step": 694, "eval/reward_0": 0.002599271758459469, "eval/reward_1": 0.003875248015873016, "eval/reward_2": -0.9375, "eval/steps": 60.0, "losses_1/value_loss": 1.1076853275299072, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.1040065586566925, "losses_1/entropy": 3.607304096221924, "losses_1/old_approx_kl": 0.06034683808684349, "losses_1/approx_kl": 0.04789261519908905, "losses_1/clipfrac": 0.2078125, "losses_1/explained_variance": -0.19167828559875488, "global_step": 640, "_wandb": {"runtime": 383}}