{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_TopoDepth": 0.0, "train/reward_SubstationSwitching": 0.0, "train/grid2opsteps": 1, "_timestamp": 1722587790.4473698, "_runtime": 192.21755170822144, "_step": 1024, "eval/reward_ScaledLinesCapacity": 0.0037392246053441396, "eval/reward_TopoDepth": -2.7023809523809463, "eval/reward_SubstationSwitching": -0.5, "eval/steps": 46.0, "losses_1/value_loss": 421.6366882324219, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.02317976951599121, "losses_1/entropy": 3.254880428314209, "losses_1/old_approx_kl": 0.1756553053855896, "losses_1/approx_kl": 0.06815845519304276, "losses_1/clipfrac": 0.253125, "losses_1/explained_variance": 0.03838539123535156, "global_step": 1024, "_wandb": {"runtime": 191}}