{"train/reward_0": 0.0, "train/reward_1": 0.6678653955459595, "train/reward_2": -0.1, "train/grid2opsteps": 310, "_timestamp": 1721286057.5140257, "_runtime": 73.68840265274048, "_step": 16, "eval/reward_0": 0.4857142857142857, "eval/reward_1": 356.3510937164227, "eval/reward_2": -51.11999999999856, "losses_1/value_loss": 118.83062744140625, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.000242576003074646, "losses_1/entropy": 3.9702725410461426, "losses_1/old_approx_kl": 0.00036412477493286133, "losses_1/approx_kl": 1.6391277313232422e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.013829350471496582, "global_step": 16, "_wandb": {"runtime": 72}}