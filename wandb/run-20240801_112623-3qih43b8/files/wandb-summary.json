{"train/reward_ScaledLinesCapacity": 0.00021398673266960196, "train/reward_ScaledDistance": 0.0004497354497354497, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 5, "_timestamp": 1722504419.4497058, "_runtime": 35.628363847732544, "_step": 108, "eval/reward_ScaledLinesCapacity": 7.903386056758085e-05, "eval/reward_ScaledDistance": 0.00012566137566137565, "eval/reward_TopoAction": -1.0, "eval/steps": 1.5, "losses_1/value_loss": 2.695399761199951, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07715269923210144, "losses_1/entropy": 3.833458423614502, "losses_1/old_approx_kl": -0.13795606791973114, "losses_1/approx_kl": 0.026480816304683685, "losses_1/clipfrac": 0.20555555820465088, "losses_1/explained_variance": -2.3251047134399414, "global_step": 108, "_wandb": {"runtime": 35}}