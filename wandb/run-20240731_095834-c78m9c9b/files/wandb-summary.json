{"train/reward_0": 1.0, "train/reward_1": 0.5934543676304265, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722413846.516255, "_runtime": 1132.2011380195618, "_step": 1024, "eval/reward_0": 0.41666666666666663, "eval/reward_1": 0.35344739891077903, "eval/reward_2": -0.375, "losses_1/value_loss": 0.5449233055114746, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.17217117547988892, "losses_1/entropy": 3.7782704830169678, "losses_1/old_approx_kl": 0.10730752348899841, "losses_1/approx_kl": 0.08800038695335388, "losses_1/clipfrac": 0.5325520833333334, "losses_1/explained_variance": 0.320640504360199, "global_step": 768, "_wandb": {"runtime": 1341}}