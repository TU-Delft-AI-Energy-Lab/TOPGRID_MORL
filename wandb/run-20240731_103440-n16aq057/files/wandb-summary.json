{"train/reward_0": 0.0, "train/reward_1": 0.00019412661622525472, "train/reward_2": -0.6666666666666666, "train/grid2opsteps": 109, "_timestamp": 1722414905.72469, "_runtime": 25.581717014312744, "_step": 64, "eval/reward_0": 0.1667906746031746, "eval/reward_1": 0.14404392014134745, "eval/reward_2": -0.7222222222222221, "eval/steps": 1009.5, "losses_1/value_loss": 3.0092201232910156, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.17940081655979156, "losses_1/entropy": 3.888676643371582, "losses_1/old_approx_kl": 0.19244647026062012, "losses_1/approx_kl": 0.13913072645664215, "losses_1/clipfrac": 0.64375, "losses_1/explained_variance": -4.552619457244873, "global_step": 64, "_wandb": {"runtime": 24}}