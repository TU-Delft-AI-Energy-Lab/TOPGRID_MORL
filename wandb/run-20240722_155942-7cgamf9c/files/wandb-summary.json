{"train/reward_0": 1.0, "train/reward_1": 0.4517138110897576, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721656888.0107017, "_runtime": 105.56051659584045, "_step": 128, "eval/reward_0": 0.01455026455026455, "eval/reward_1": 0.027048024706702714, "eval/reward_2": -0.3333333333333333, "losses_1/value_loss": 0.31606215238571167, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0002290382981300354, "losses_1/entropy": 3.970203399658203, "losses_1/old_approx_kl": -9.641051292419434e-05, "losses_1/approx_kl": 2.8777867555618286e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.5558779239654541, "global_step": 128, "_wandb": {"runtime": 104}}