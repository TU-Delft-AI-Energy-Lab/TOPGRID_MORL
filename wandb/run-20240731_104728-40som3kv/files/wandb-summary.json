{"train/reward_0": 0.023313492063492064, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 94, "_timestamp": 1722416088.3305202, "_runtime": 440.27380108833313, "_step": 851, "eval/reward_0": 0.5001240079365079, "eval/reward_1": 0.3188387315101884, "eval/reward_2": -0.41666666666666663, "eval/steps": 1009.0, "losses_1/value_loss": 0.4052877128124237, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.1410776674747467, "losses_1/entropy": 3.7671899795532227, "losses_1/old_approx_kl": 0.07138757407665253, "losses_1/approx_kl": 0.09108077734708786, "losses_1/clipfrac": 0.55625, "losses_1/explained_variance": -0.0803065299987793, "global_step": 768, "_wandb": {"runtime": 440}}