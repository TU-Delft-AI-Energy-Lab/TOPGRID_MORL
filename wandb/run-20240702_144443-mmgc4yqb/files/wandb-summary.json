{"losses_1/value_loss": 156.66085815429688, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.08337576687335968, "losses_1/entropy": 4.594721794128418, "losses_1/old_approx_kl": -0.08412208408117294, "losses_1/approx_kl": 0.008073377422988415, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.002364039421081543, "global_step": 5, "_timestamp": 1719924285.6096337, "_runtime": 1.6295287609100342, "_step": 1, "charts_1/episode_reward_sum": 43.688385951518995, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.0, "charts_1/episode_reward_1": 37.38838595151901, "charts_1/episode_reward_2": 6.2999999999999865, "_wandb": {"runtime": 0}}