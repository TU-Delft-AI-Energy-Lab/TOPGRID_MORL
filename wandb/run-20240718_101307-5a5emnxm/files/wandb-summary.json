{"train/reward_0": 0.04563492063492063, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 362, "_timestamp": 1721290483.3258011, "_runtime": 95.33165717124939, "_step": 32, "eval/reward_0": 0.21927910052910052, "eval/reward_1": 0.1293878541622412, "eval/reward_2": 230.4, "losses_1/value_loss": 291386.5, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.0001340806484222412, "losses_1/entropy": 3.970242977142334, "losses_1/old_approx_kl": -3.0875205993652344e-05, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -3.826618194580078e-05, "global_step": 32, "_wandb": {"runtime": 94}}