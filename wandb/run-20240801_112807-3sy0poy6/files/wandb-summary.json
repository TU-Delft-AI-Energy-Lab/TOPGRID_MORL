{"train/reward_ScaledLinesCapacity": 0.02454370604858799, "train/reward_ScaledDistance": 0.04724867724867726, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 94, "_timestamp": 1722504516.4462469, "_runtime": 29.03310489654541, "_step": 52, "eval/reward_ScaledLinesCapacity": 0.5204274889847653, "eval/reward_ScaledDistance": 0.6044797178130639, "eval/reward_TopoAction": -0.3333333333333333, "eval/steps": 2016.0, "losses_1/value_loss": 18.64951515197754, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.09946528822183609, "losses_1/entropy": 3.9448251724243164, "losses_1/old_approx_kl": 0.0562705472111702, "losses_1/approx_kl": 0.06081122159957886, "losses_1/clipfrac": 0.43333333283662795, "losses_1/explained_variance": -23.148250579833984, "global_step": 36, "_wandb": {"runtime": 29}}