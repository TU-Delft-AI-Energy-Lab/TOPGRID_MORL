{"train/reward_0": 0.0, "train/reward_1": 0.00032546015423337343, "train/reward_2": 0.0, "train/grid2opsteps": 93, "_timestamp": 1722336693.7489235, "_runtime": 6715.095684528351, "_step": 1024, "eval/reward_0": 0.048098544973544974, "eval/reward_1": 0.04983573260527973, "eval/reward_2": -0.5777777777777777, "losses_1/value_loss": 0.49710169434547424, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": -0.002252122387290001, "losses_1/entropy": 3.963989496231079, "losses_1/old_approx_kl": -0.000811263918876648, "losses_1/approx_kl": 8.014962077140808e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.47445857524871826, "global_step": 1024, "_wandb": {"runtime": 6714}}