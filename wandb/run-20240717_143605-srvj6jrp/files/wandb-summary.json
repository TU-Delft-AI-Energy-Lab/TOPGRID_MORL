{"train/reward_0": 2.0, "train/reward_1": -972.392034110422, "train/reward_2": -185.79999999999373, "train/grid2opsteps": 2410, "_timestamp": 1721219824.7695887, "_runtime": 58.80864977836609, "_step": 16, "eval/reward_0": 0.08278769841269842, "eval/reward_1": -53.43775684239113, "eval/reward_2": -10.38333333333326, "losses_1/value_loss": 150852.328125, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 9.119510650634766e-06, "losses_1/entropy": 3.970266103744507, "losses_1/old_approx_kl": -0.00012797117233276367, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0018468499183654785, "global_step": 16, "_wandb": {"runtime": 58}}