{"train/reward_0": 0.0, "train/reward_1": 0.06823533905434488, "train/reward_2": 149.0, "train/grid2opsteps": 149, "_timestamp": 1722417131.8671927, "_runtime": 26.583573818206787, "_step": 64, "eval/reward_0": 0.5001240079365079, "eval/reward_1": 0.3931900789156626, "eval/reward_2": 1007.75, "eval/steps": 1009.0, "losses_1/value_loss": 173262.296875, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15584459900856018, "losses_1/entropy": 3.8805923461914062, "losses_1/old_approx_kl": -0.13796447217464447, "losses_1/approx_kl": 0.2381492406129837, "losses_1/clipfrac": 0.5166666666666667, "losses_1/explained_variance": 0.0033649206161499023, "global_step": 64, "_wandb": {"runtime": 26}}