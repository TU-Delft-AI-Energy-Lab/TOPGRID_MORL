{"train/reward_0": 1.0, "train/reward_1": 0.594044357832273, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722347557.5688598, "_runtime": 60.313164710998535, "_step": 32, "eval/reward_0": 0.20819279100529098, "eval/reward_1": 0.1720985716450834, "eval/reward_2": -0.5166666666666667, "losses_1/value_loss": 0.9386577010154724, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.019887445494532585, "losses_1/entropy": 3.970233678817749, "losses_1/old_approx_kl": 0.00577661395072937, "losses_1/approx_kl": 0.0002720952033996582, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.23370492458343506, "global_step": 16, "_wandb": {"runtime": 62}}