{"train/reward_0": 0.00028776374779628194, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 151, "_timestamp": 1722431245.9391518, "_runtime": 369.8668327331543, "_step": 652, "eval/reward_0": 0.1667906746031746, "eval/reward_1": 0.3335813492063492, "eval/reward_2": -0.8333333333333333, "eval/steps": 1010.5, "losses_1/value_loss": 0.8123385310173035, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07257638871669769, "losses_1/entropy": 3.539672374725342, "losses_1/old_approx_kl": -0.013896003365516663, "losses_1/approx_kl": 0.03213243559002876, "losses_1/clipfrac": 0.1796875, "losses_1/explained_variance": 0.17984074354171753, "global_step": 640, "_wandb": {"runtime": 371}}