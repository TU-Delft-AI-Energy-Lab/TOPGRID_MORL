{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledTopoDepth": 0.0, "train/reward_SubstationSwitching": 0.0, "train/grid2opsteps": 3, "_timestamp": 1722673759.8392282, "_runtime": 416.06266713142395, "_step": 423, "eval/reward_ScaledLinesCapacity": 0.17761115228919777, "eval/reward_ScaledTopoDepth": -0.12220899470899747, "eval/reward_SubstationSwitching": -2.25, "eval/steps": 1177.0, "losses_1/value_loss": 45.08292007446289, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.11663071811199188, "losses_1/entropy": 3.770761013031006, "losses_1/old_approx_kl": -0.033504411578178406, "losses_1/approx_kl": 0.02681235410273075, "losses_1/clipfrac": 0.134375, "losses_1/explained_variance": 0.01202535629272461, "global_step": 384, "_wandb": {"runtime": 415}}