{"train/reward_0": 0.025297619047619048, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 788, "_timestamp": 1721644451.8808203, "_runtime": 15.276044368743896, "_step": 16, "eval/reward_0": 0.08271329365079365, "eval/reward_1": 0.08217603223035504, "eval/reward_2": -0.13333333333333333, "losses_1/value_loss": 0.3475073575973511, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0007299706339836121, "losses_1/entropy": 3.9702467918395996, "losses_1/old_approx_kl": -0.001256108283996582, "losses_1/approx_kl": 1.519918441772461e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.04878515005111694, "global_step": 16, "_wandb": {"runtime": 13}}