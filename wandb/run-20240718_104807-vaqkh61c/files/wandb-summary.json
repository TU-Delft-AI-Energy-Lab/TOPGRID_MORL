{"train/reward_0": 0.000248015873015873, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721292532.1221828, "_runtime": 44.42575192451477, "_step": 24, "eval/reward_0": 0.07748842592592592, "eval/reward_1": 0.08392245933217385, "eval/reward_2": -Infinity, "losses_1/value_loss": 1519.02490234375, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -0.0006006956100463867, "losses_1/entropy": 3.9702584743499756, "losses_1/old_approx_kl": 0.00015932321548461914, "losses_1/approx_kl": 3.5762786865234375e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.001980900764465332, "global_step": 16, "_wandb": {"runtime": 64}}