{"train/reward_0": 0.0, "train/reward_1": 9.502608806697158e-05, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721738319.4733238, "_runtime": 142.57050490379333, "_step": 128, "eval/reward_0": 0.11236772486772487, "eval/reward_1": 0.10458858742931977, "eval/reward_2": -0.4388888888888889, "losses_1/value_loss": 0.23992250859737396, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": 4.139542579650879e-05, "losses_1/entropy": 3.970158100128174, "losses_1/old_approx_kl": -2.562999725341797e-05, "losses_1/approx_kl": 1.4901161193847656e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.272208571434021, "global_step": 128, "_wandb": {"runtime": 140}}