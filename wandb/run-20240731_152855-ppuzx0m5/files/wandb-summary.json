{"train/reward_0": 0.00016899431884478565, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722432737.7415829, "_runtime": 201.7723319530487, "_step": 448, "eval/reward_0": 6.242359858830464e-05, "eval/reward_1": 0.000496031746031746, "eval/reward_2": -1.0, "eval/steps": 1.5, "losses_1/value_loss": 1.4938188791275024, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15134690701961517, "losses_1/entropy": 3.9088833332061768, "losses_1/old_approx_kl": 0.0812082290649414, "losses_1/approx_kl": 0.08252216875553131, "losses_1/clipfrac": 0.61171875, "losses_1/explained_variance": -0.30457448959350586, "global_step": 256, "_wandb": {"runtime": 201}}