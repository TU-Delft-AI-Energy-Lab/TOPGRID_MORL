{"train/reward_0": 0.00013492193340966762, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 496, "_timestamp": 1722432507.3720398, "_runtime": 95.12428569793701, "_step": 256, "eval/reward_0": 0.14118004863561268, "eval/reward_1": 0.3381944444444444, "eval/reward_2": -0.7333333333333334, "eval/steps": 1057.0, "losses_1/value_loss": 7.535259246826172, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.09601934254169464, "losses_1/entropy": 3.906371831893921, "losses_1/old_approx_kl": 0.00126570463180542, "losses_1/approx_kl": 0.07195303589105606, "losses_1/clipfrac": 0.45, "losses_1/explained_variance": -5.621623992919922, "global_step": 128, "_wandb": {"runtime": 96}}