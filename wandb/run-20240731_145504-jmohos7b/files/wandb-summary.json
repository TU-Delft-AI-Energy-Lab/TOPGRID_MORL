{"train/reward_0": 0.9387487294882564, "train/reward_1": 1645.9951696023345, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722430852.735931, "_runtime": 348.6985619068146, "_step": 596, "eval/reward_0": 0.6666666666666666, "eval/reward_1": 1085.9779778483014, "eval/reward_2": -0.16666666666666666, "eval/steps": 2016.0, "losses_1/value_loss": 232260.046875, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.08058413863182068, "losses_1/entropy": 3.6127123832702637, "losses_1/old_approx_kl": 0.07980288565158844, "losses_1/approx_kl": 0.03146122023463249, "losses_1/clipfrac": 0.2, "losses_1/explained_variance": 0.006075918674468994, "global_step": 512, "_wandb": {"runtime": 348}}