{"train/reward_0": 0.0, "train/reward_1": 0.0002847743927866059, "train/reward_2": -0.6666666666666666, "train/grid2opsteps": 231, "_timestamp": 1721739470.8766367, "_runtime": 168.124205827713, "_step": 128, "eval/reward_0": 0.2089285714285714, "eval/reward_1": 0.18601707216874405, "eval/reward_2": -0.49444444444444446, "losses_1/value_loss": 1.4105535745620728, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": -7.975473999977112e-05, "losses_1/entropy": 3.969963550567627, "losses_1/old_approx_kl": 0.00021257996559143066, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.11038422584533691, "global_step": 128, "_wandb": {"runtime": 167}}