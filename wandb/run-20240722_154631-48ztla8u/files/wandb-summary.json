{"train/reward_0": 0.0, "train/reward_1": 0.0002348219299561056, "train/reward_2": 0.0, "train/grid2opsteps": 1293, "_timestamp": 1721656113.4034739, "_runtime": 121.64798784255981, "_step": 128, "eval/reward_0": 0.056712962962962965, "eval/reward_1": 0.1025907533142299, "eval/reward_2": -0.3333333333333333, "losses_1/value_loss": 0.18106289207935333, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 8.389912545681e-05, "losses_1/entropy": 3.9701619148254395, "losses_1/old_approx_kl": 5.7250261306762695e-05, "losses_1/approx_kl": 3.241002559661865e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.23731601238250732, "global_step": 128, "_wandb": {"runtime": 120}}