{"train/reward_ScaledLinesCapacity": 0.02447653967296453, "train/reward_ScaledDistance": 0.05166666666666655, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 93, "_timestamp": 1722507454.4841979, "_runtime": 37.3110888004303, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.4187359948541021, "eval/reward_ScaledDistance": 0.4800000000000157, "eval/reward_TopoAction": -0.5, "eval/steps": 1008.5, "losses_1/value_loss": 1.0452135801315308, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.12081653624773026, "losses_1/entropy": 3.814621925354004, "losses_1/old_approx_kl": 0.20864413678646088, "losses_1/approx_kl": 0.06165527552366257, "losses_1/clipfrac": 0.2888888940215111, "losses_1/explained_variance": -0.40830492973327637, "global_step": 108, "_wandb": {"runtime": 36}}