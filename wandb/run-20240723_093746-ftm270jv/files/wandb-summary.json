{"train/reward_0": 0.0, "train/reward_1": 0.0002702913964277228, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1721720394.883261, "_runtime": 128.8296618461609, "_step": 128, "eval/reward_0": 0.2, "eval/reward_1": 0.18221300115458122, "eval/reward_2": 0.0, "losses_1/value_loss": 0.20456300675868988, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0006844233721494675, "losses_1/entropy": 3.9702165126800537, "losses_1/old_approx_kl": 0.0001987740397453308, "losses_1/approx_kl": 2.333894371986389e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -1.6501390933990479, "global_step": 128, "_wandb": {"runtime": 127}}