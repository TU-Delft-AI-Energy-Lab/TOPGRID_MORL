{"train/reward_0": 0.0, "train/reward_1": 0.04531214969632651, "train/reward_2": 0.0, "train/grid2opsteps": 121, "_timestamp": 1721370201.5703413, "_runtime": 58.624059438705444, "_step": 16, "eval/reward_0": 0.14229497354497356, "eval/reward_1": 0.10715298514315093, "eval/reward_2": -0.3, "losses_1/value_loss": 0.29071906208992004, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.00013224780559539795, "losses_1/entropy": 3.97025990486145, "losses_1/old_approx_kl": -5.042552947998047e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -1.5461711883544922, "global_step": 16, "_wandb": {"runtime": 57}}