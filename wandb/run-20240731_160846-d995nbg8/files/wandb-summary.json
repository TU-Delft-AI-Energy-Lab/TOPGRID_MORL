{"train/reward_0": 0.0, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722435116.7249365, "_runtime": 190.53040146827698, "_step": 418, "eval/reward_0": 0.011018077846498272, "eval/reward_1": 24.763888888888896, "eval/reward_2": -0.7083333333333333, "eval/steps": 106.0, "losses_1/value_loss": 100594.1171875, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.02110440284013748, "losses_1/entropy": 3.8584961891174316, "losses_1/old_approx_kl": -0.012011099606752396, "losses_1/approx_kl": 0.011987783946096897, "losses_1/clipfrac": 0.1125, "losses_1/explained_variance": 0.008483171463012695, "global_step": 384}