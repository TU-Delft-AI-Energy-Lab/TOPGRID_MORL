{"train/reward_0": 0.0, "train/reward_1": 0.0002754443607792176, "train/reward_2": 0.0, "train/grid2opsteps": 2611, "_timestamp": 1721644426.0099468, "_runtime": 30.860971927642822, "_step": 16, "eval/reward_0": 0.13421792328042326, "eval/reward_1": 0.1174687957214582, "eval/reward_2": -0.1, "losses_1/value_loss": 0.014279275201261044, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0005916878581047058, "losses_1/entropy": 3.9702625274658203, "losses_1/old_approx_kl": -0.0029333829879760742, "losses_1/approx_kl": 5.602836608886719e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.40896761417388916, "global_step": 16, "_wandb": {"runtime": 29}}