{"train/reward_0": 0.0, "train/reward_1": 0.0002068244232651501, "train/reward_2": -0.6666666666666666, "train/grid2opsteps": 1, "_timestamp": 1721724795.0784965, "_runtime": 149.42684745788574, "_step": 128, "eval/reward_0": 0.000248015873015873, "eval/reward_1": 0.00021900307556297979, "eval/reward_2": -0.6111111111111112, "losses_1/value_loss": 0.5798839926719666, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.00016373954713344574, "losses_1/entropy": 3.9702224731445312, "losses_1/old_approx_kl": -0.00023312121629714966, "losses_1/approx_kl": 1.607462763786316e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.18539273738861084, "global_step": 128, "_wandb": {"runtime": 148}}