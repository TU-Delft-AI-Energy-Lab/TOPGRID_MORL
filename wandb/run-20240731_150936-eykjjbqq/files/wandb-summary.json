{"train/reward_0": 1554.940983965993, "train/reward_1": 2.0, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722431899.5793781, "_runtime": 523.2493672370911, "_step": 1024, "eval/reward_0": 842.1552647352219, "eval/reward_1": 1.0002480158730158, "eval/reward_2": -0.5, "eval/steps": 1008.5, "losses_1/value_loss": 215943.015625, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.06303787231445312, "losses_1/entropy": 3.567718505859375, "losses_1/old_approx_kl": -0.007781416177749634, "losses_1/approx_kl": 0.013447210192680359, "losses_1/clipfrac": 0.046875, "losses_1/explained_variance": 0.008589327335357666, "global_step": 1024, "_wandb": {"runtime": 522}}