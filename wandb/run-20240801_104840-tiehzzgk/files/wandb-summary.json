{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledDistance": 0.0, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 5, "_timestamp": 1722502157.6783595, "_runtime": 36.87883639335632, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.5602297229448477, "eval/reward_ScaledDistance": 0.6133509700176519, "eval/reward_TopoAction": -0.3333333333333333, "eval/steps": 2016.0, "losses_1/value_loss": 3.390760898590088, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.09134315699338913, "losses_1/entropy": 3.8988707065582275, "losses_1/old_approx_kl": -0.04021602123975754, "losses_1/approx_kl": 0.031573906540870667, "losses_1/clipfrac": 0.1833333346992731, "losses_1/explained_variance": -4.924412727355957, "global_step": 108, "_wandb": {"runtime": 36}}