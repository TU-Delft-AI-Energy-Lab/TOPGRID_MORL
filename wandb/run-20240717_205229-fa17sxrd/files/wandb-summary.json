{"train/reward_0": 0.000992063492063492, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 3, "_timestamp": 1721242373.5261066, "_runtime": 24.450145483016968, "_step": 16, "eval/reward_0": 0.28131613756613755, "eval/reward_1": 227.9574185828368, "eval/reward_2": -30.38666666666592, "losses_1/value_loss": 369.9390869140625, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 3.62396240234375e-05, "losses_1/entropy": 3.9702742099761963, "losses_1/old_approx_kl": -6.181001663208008e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.004943549633026123, "global_step": 16, "_wandb": {"runtime": 23}}