{"train/reward_0": 0.0, "train/reward_1": 0.033098478606068744, "train/reward_2": 0.0, "train/grid2opsteps": 94, "_timestamp": 1722410203.0079007, "_runtime": 73.8799958229065, "_step": 64, "losses_1/value_loss": 0.8422863483428955, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0032601170241832733, "losses_1/entropy": 3.9698586463928223, "losses_1/old_approx_kl": -0.0017414987087249756, "losses_1/approx_kl": 1.843273639678955e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.28173160552978516, "global_step": 48, "_wandb": {"runtime": 80}}