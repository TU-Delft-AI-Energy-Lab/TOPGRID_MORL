{"losses_1/value_loss": 1129987.75, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.17325910925865173, "losses_1/entropy": 4.492517948150635, "losses_1/old_approx_kl": 0.021570095792412758, "losses_1/approx_kl": 0.09460122138261795, "losses_1/clipfrac": 0.8867931544780732, "losses_1/explained_variance": 0.006224691867828369, "global_step": 348, "_timestamp": 1719930083.2599132, "_runtime": 102.65837526321411, "_step": 199, "charts_1/episode_reward_sum": -0.14401803641092215, "charts_1/episode": 99, "charts_1/episode_reward_0": 0.001488095238095238, "charts_1/episode_reward_1": 0.6544938683509827, "charts_1/episode_reward_2": -0.8, "_wandb": {"runtime": 101}}