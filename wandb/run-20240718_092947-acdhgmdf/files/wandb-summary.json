{"train/reward_0": 0.000496031746031746, "train/reward_1": 177.97311401367188, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721287944.4496129, "_runtime": 156.92136478424072, "_step": 32, "eval/reward_0": 78.65748884999563, "eval/reward_1": -198.27406498125404, "eval/reward_2": -30.923333333332593, "losses_1/value_loss": 146648.703125, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 3.1113624572753906e-05, "losses_1/entropy": 3.970257043838501, "losses_1/old_approx_kl": -6.252527236938477e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0001703500747680664, "global_step": 32, "_wandb": {"runtime": 156}}