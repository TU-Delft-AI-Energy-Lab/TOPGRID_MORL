{"losses_1/value_loss": 1791.05078125, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.08106324076652527, "losses_1/entropy": 4.594913959503174, "losses_1/old_approx_kl": 0.039698123931884766, "losses_1/approx_kl": 0.00468101492151618, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0012034773826599121, "global_step": 5, "_timestamp": 1719924293.2197053, "_runtime": 2.3033063411712646, "_step": 1, "charts_1/episode_reward_sum": 271.33895239234, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.0, "charts_1/episode_reward_1": 232.8389523923397, "charts_1/episode_reward_2": 38.50000000000028, "_wandb": {"runtime": 1}}