{"train/reward_0": 0.0, "train/reward_1": 0.00042206794417263253, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1722417020.3333101, "_runtime": 21.869367122650146, "_step": 64, "eval/reward_0": 0.000248015873015873, "eval/reward_1": 0.00014274857697699957, "eval/reward_2": -0.375, "eval/steps": 3.0, "losses_1/value_loss": 181125.984375, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15826177597045898, "losses_1/entropy": 3.873419761657715, "losses_1/old_approx_kl": -0.08644634485244751, "losses_1/approx_kl": 0.1948053240776062, "losses_1/clipfrac": 0.5895833333333333, "losses_1/explained_variance": 0.0058019161224365234, "global_step": 64, "_wandb": {"runtime": 21}}