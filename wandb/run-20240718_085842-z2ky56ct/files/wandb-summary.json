{"train/reward_0": 0.001488095238095238, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 6, "_timestamp": 1721285959.48234, "_runtime": 37.02790307998657, "_step": 16, "eval/reward_0": 0.10046296296296295, "eval/reward_1": 100.6052044433852, "eval/reward_2": -13.749999999999803, "losses_1/value_loss": 181324.25, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0009608417749404907, "losses_1/entropy": 3.970264434814453, "losses_1/old_approx_kl": 0.0007292628288269043, "losses_1/approx_kl": 9.387731552124023e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0007309913635253906, "global_step": 16, "_wandb": {"runtime": 36}}