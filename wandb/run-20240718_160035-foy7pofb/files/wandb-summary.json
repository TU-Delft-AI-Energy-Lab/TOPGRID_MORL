{"train/reward_0": 0.000248015873015873, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721311341.756911, "_runtime": 106.70038199424744, "_step": 32, "eval/reward_0": 0.10902777777777777, "eval/reward_1": 0.10111087878492864, "eval/reward_2": -0.2, "losses_1/value_loss": 0.1933644562959671, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 6.687641143798828e-05, "losses_1/entropy": 3.9702517986297607, "losses_1/old_approx_kl": 0.0001480579376220703, "losses_1/approx_kl": 1.341104507446289e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.12845689058303833, "global_step": 32, "_wandb": {"runtime": 105}}