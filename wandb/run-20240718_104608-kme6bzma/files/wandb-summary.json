{"train/reward_0": 0.0, "train/reward_1": 0.00034417933919791033, "train/reward_2": -60.0, "train/grid2opsteps": 3, "_timestamp": 1721292470.3855953, "_runtime": 101.75349640846252, "_step": 32, "eval/reward_0": 0.07251157407407406, "eval/reward_1": 0.0691535761436385, "eval/reward_2": -Infinity, "losses_1/value_loss": 382.0368347167969, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 5.303323268890381e-05, "losses_1/entropy": 3.9702255725860596, "losses_1/old_approx_kl": 0.00013011693954467773, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.004011213779449463, "global_step": 32, "_wandb": {"runtime": 100}}