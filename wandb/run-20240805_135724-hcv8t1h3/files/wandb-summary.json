{"train/reward_ScaledLinesCapacity": 0.0002172929787548108, "train/reward_L2RPN": 4.213759422302246, "train/reward_ScaledTopoDepth": -0.0002857142857142857, "train/grid2opsteps": 1, "_timestamp": 1722859286.9546528, "_runtime": 242.5858907699585, "_step": 376, "eval/reward_ScaledLinesCapacity": 0.6113137615835348, "eval/reward_L2RPN": 10786.96855968237, "eval/reward_ScaledTopoDepth": -0.5279285714285812, "eval/steps": 2016.0, "losses_1/value_loss": 21797324.0, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.08049675077199936, "losses_1/entropy": 3.538632869720459, "losses_1/old_approx_kl": 0.01947122812271118, "losses_1/approx_kl": 0.04550500214099884, "losses_1/clipfrac": 0.20625, "losses_1/explained_variance": 0.0008830428123474121, "global_step": 320, "_wandb": {"runtime": 242}}