{"train/reward_0": 0.0, "train/reward_1": 0.00023452057408991832, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1721310935.4329646, "_runtime": 132.94189548492432, "_step": 32, "eval/reward_0": 0.10498511904761905, "eval/reward_1": 0.09276772683225615, "eval/reward_2": -0.13333333333333333, "losses_1/value_loss": 0.08726570755243301, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -9.226799011230469e-05, "losses_1/entropy": 3.9702391624450684, "losses_1/old_approx_kl": -2.300739288330078e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.5180014371871948, "global_step": 32, "_wandb": {"runtime": 132}}