{"train/reward_0": 0.0, "train/reward_1": 0.00042853818889555323, "train/reward_2": 0.0, "train/grid2opsteps": 1, "_timestamp": 1721655455.0986114, "_runtime": 182.23294138908386, "_step": 128, "eval/reward_0": 0.10892030423280422, "eval/reward_1": 0.0995993609675125, "eval/reward_2": -0.26666666666666666, "losses_1/value_loss": 0.5194478034973145, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -5.262717604637146e-05, "losses_1/entropy": 3.9701685905456543, "losses_1/old_approx_kl": -1.735985279083252e-05, "losses_1/approx_kl": 2.682209014892578e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -2.6550252437591553, "global_step": 128, "_wandb": {"runtime": 181}}