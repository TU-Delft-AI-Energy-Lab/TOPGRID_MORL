{"train/reward_0": 0.03521825396825397, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 142, "_timestamp": 1722346879.3154998, "_runtime": 326.1110727787018, "_step": 646, "eval/reward_0": 0.104265873015873, "eval/reward_1": 0.08165079980830672, "eval/reward_2": -0.43888888888888894, "reward": 1.9439250728723279, "steps": 2016, "losses_1/value_loss": 0.21801112592220306, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.007973715662956238, "losses_1/entropy": 3.96689510345459, "losses_1/old_approx_kl": 0.00393792986869812, "losses_1/approx_kl": 0.000133514404296875, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.5236085951328278, "global_step": 640, "_wandb": {"runtime": 327}}