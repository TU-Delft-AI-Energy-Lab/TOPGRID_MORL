{"train/reward_0": 1.0, "train/reward_1": 0.415098140113641, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721723153.7783978, "_runtime": 107.4795138835907, "_step": 121, "eval/reward_0": 0.000248015873015873, "eval/reward_1": 0.0, "eval/reward_2": 0.0, "losses_1/value_loss": 0.6147685647010803, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.00019171833992004395, "losses_1/entropy": 3.970247745513916, "losses_1/old_approx_kl": 0.0009893327951431274, "losses_1/approx_kl": 2.4545937776565552e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.8675880432128906, "global_step": 64, "_wandb": {"runtime": 95}}