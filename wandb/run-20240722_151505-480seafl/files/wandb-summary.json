{"train/reward_0": 0.0, "train/reward_1": 0.00037998797417336945, "train/reward_2": 0.0, "train/grid2opsteps": 136, "_timestamp": 1721654151.6506753, "_runtime": 46.2064642906189, "_step": 16, "eval/reward_0": 0.08169642857142857, "eval/reward_1": 0.07772562888966102, "eval/reward_2": -0.23333333333333334, "losses_1/value_loss": 0.26762646436691284, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.00010889768600463867, "losses_1/entropy": 3.9702720642089844, "losses_1/old_approx_kl": -0.0007072687149047852, "losses_1/approx_kl": 2.980232238769531e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.709372878074646, "global_step": 8, "_wandb": {"runtime": 73}}