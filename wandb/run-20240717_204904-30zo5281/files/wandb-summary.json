{"train/reward_0": 0.0, "train/reward_1": 0.4455268979072571, "train/reward_2": -0.1, "train/grid2opsteps": 1490, "_timestamp": 1721242172.2655227, "_runtime": 27.753153800964355, "_step": 16, "eval/reward_0": 0.2749669312169312, "eval/reward_1": 229.05708711966872, "eval/reward_2": -30.099999999999294, "losses_1/value_loss": 2115.052490234375, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 9.66191291809082e-05, "losses_1/entropy": 3.9702610969543457, "losses_1/old_approx_kl": -0.0005130171775817871, "losses_1/approx_kl": 7.152557373046875e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0020241737365722656, "global_step": 16, "_wandb": {"runtime": 27}}