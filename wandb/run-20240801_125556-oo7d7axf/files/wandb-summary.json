{"train/reward_ScaledLinesCapacity": 0.00024808468151265824, "train/reward_ScaledDistance": 0.0004497354497354497, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 2, "_timestamp": 1722510096.3354461, "_runtime": 340.0371630191803, "_step": 595, "eval/reward_ScaledLinesCapacity": 0.4840050961278021, "eval/reward_ScaledDistance": 0.48574074074075646, "eval/reward_TopoAction": -1.0, "eval/steps": 1069.0, "losses_1/value_loss": 0.6452909111976624, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.08183872699737549, "losses_1/entropy": 3.509481906890869, "losses_1/old_approx_kl": 0.19215598702430725, "losses_1/approx_kl": 0.05684225261211395, "losses_1/clipfrac": 0.2125, "losses_1/explained_variance": 0.4399026036262512, "global_step": 576, "_wandb": {"runtime": 341}}