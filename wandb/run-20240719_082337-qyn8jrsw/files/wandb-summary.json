{"train/reward_0": 0.0, "train/reward_1": 8.46846017513445e-05, "train/reward_2": 0.0, "train/grid2opsteps": 355, "_timestamp": 1721370279.017155, "_runtime": 61.45025587081909, "_step": 16, "eval/reward_0": 0.08528439153439153, "eval/reward_1": 0.08385345856055007, "eval/reward_2": -0.3, "losses_1/value_loss": 0.04801671579480171, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.00024076923727989197, "losses_1/entropy": 3.9702539443969727, "losses_1/old_approx_kl": 0.0005815625190734863, "losses_1/approx_kl": 2.2351741790771484e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.5951076149940491, "global_step": 16, "_wandb": {"runtime": 60}}