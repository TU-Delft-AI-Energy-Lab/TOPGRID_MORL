{"losses_1/value_loss": 365.7608642578125, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.0996255949139595, "losses_1/entropy": 4.594712257385254, "losses_1/old_approx_kl": -0.04669637605547905, "losses_1/approx_kl": 0.008538532070815563, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.004205226898193359, "global_step": 5, "_timestamp": 1719924276.6518633, "_runtime": 1.2702932357788086, "_step": 1, "charts_1/episode_reward_sum": 112.22802698314187, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.0, "charts_1/episode_reward_1": 98.5280269831419, "charts_1/episode_reward_2": 13.699999999999976, "_wandb": {"runtime": 0}}