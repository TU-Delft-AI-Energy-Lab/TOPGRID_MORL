{"train/reward_ScaledLinesCapacity": 0.00038076995270330057, "train/reward_EpisodeDuration": 0.0, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 102, "_timestamp": 1722460311.7112, "_runtime": 1265.5038690567017, "_step": 1016, "eval/reward_ScaledLinesCapacity": 0.08082947003687847, "eval/reward_EpisodeDuration": 0.25024801587301587, "eval/reward_TopoAction": -0.625, "eval/steps": 1009.0, "losses_1/value_loss": 0.4617055356502533, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.1393987536430359, "losses_1/entropy": 3.783740997314453, "losses_1/old_approx_kl": 0.06914213299751282, "losses_1/approx_kl": 0.09088575839996338, "losses_1/clipfrac": 0.5703125, "losses_1/explained_variance": 0.4004489779472351, "global_step": 768}