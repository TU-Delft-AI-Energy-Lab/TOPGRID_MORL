{"train/reward_0": 0.0, "train/reward_1": 81.23773521184921, "train/reward_2": -10.699999999999978, "train/grid2opsteps": 112, "_timestamp": 1721287402.5819848, "_runtime": 112.54676270484924, "_step": 32, "eval/reward_0": 0.14188161375661373, "eval/reward_1": 116.69581245027838, "eval/reward_2": -15.669999999999725, "losses_1/value_loss": 472.336181640625, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -5.14984130859375e-05, "losses_1/entropy": 3.9702608585357666, "losses_1/old_approx_kl": -0.00010502338409423828, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0034781694412231445, "global_step": 32, "_wandb": {"runtime": 111}}