{"train/reward_0": 0.255332350730896, "train/reward_1": -0.7645723495921763, "train/reward_2": -0.1, "train/grid2opsteps": 1, "_timestamp": 1721288216.385625, "_runtime": 118.01519179344177, "_step": 24, "eval/reward_0": 99.92299476060917, "eval/reward_1": -243.7847941572925, "eval/reward_2": -39.326666666665524, "losses_1/value_loss": 356045.71875, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0004150122404098511, "losses_1/entropy": 3.970261812210083, "losses_1/old_approx_kl": 0.0004271268844604492, "losses_1/approx_kl": 4.6193599700927734e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0005577802658081055, "global_step": 24, "_wandb": {"runtime": 116}}