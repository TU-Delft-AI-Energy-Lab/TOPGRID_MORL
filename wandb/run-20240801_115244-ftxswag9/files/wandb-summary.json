{"train/reward_ScaledLinesCapacity": 0.0001954794372123714, "train/reward_ScaledDistance": 0.0005555555555555556, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 85, "_timestamp": 1722505999.1032236, "_runtime": 34.58557462692261, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.3240224828307913, "eval/reward_ScaledDistance": 0.46486111111111617, "eval/reward_TopoAction": -0.25, "eval/steps": 1050.0, "losses_1/value_loss": 121541.7890625, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.00765588553622365, "losses_1/entropy": 3.8866238594055176, "losses_1/old_approx_kl": -0.03121936321258545, "losses_1/approx_kl": 0.015464656986296177, "losses_1/clipfrac": 0.11111111380159855, "losses_1/explained_variance": 0.007421433925628662, "global_step": 108, "_wandb": {"runtime": 34}}