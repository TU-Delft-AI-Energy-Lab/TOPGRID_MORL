{"train/reward_0": 0.0, "train/reward_1": 0.030879613907000617, "train/reward_2": 0.0, "train/grid2opsteps": 87, "_timestamp": 1721656376.134433, "_runtime": 158.8371880054474, "_step": 128, "eval/reward_0": 1.0, "eval/reward_1": 0.8626268489717906, "eval/reward_2": 0.0, "losses_1/value_loss": 0.1674107164144516, "charts_1/learning_rate": 1.875e-05, "losses_1/policy_loss": -0.00012305378913879395, "losses_1/entropy": 3.9696884155273438, "losses_1/old_approx_kl": 9.810924530029297e-05, "losses_1/approx_kl": 1.4901161193847656e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.20556950569152832, "global_step": 128, "_wandb": {"runtime": 157}}