{"train/reward_0": 0.0, "train/reward_1": 0.04957296171687011, "train/reward_2": 0.0, "train/grid2opsteps": 99, "_timestamp": 1722417338.7320096, "_runtime": 35.943949699401855, "_step": 64, "eval/reward_0": 0.25012400793650796, "eval/reward_1": 0.2188397031988644, "eval/reward_2": -0.5, "eval/steps": 1008.5, "losses_1/value_loss": 2.9712531566619873, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.159143328666687, "losses_1/entropy": 3.922380208969116, "losses_1/old_approx_kl": 0.17136012017726898, "losses_1/approx_kl": 0.15787285566329956, "losses_1/clipfrac": 0.64375, "losses_1/explained_variance": -1.2789196968078613, "global_step": 64, "_wandb": {"runtime": 35}}