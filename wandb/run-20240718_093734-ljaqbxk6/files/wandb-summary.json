{"train/reward_0": 0.001488095238095238, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 6, "_timestamp": 1721288336.2343838, "_runtime": 81.7587468624115, "_step": 32, "eval/reward_0": 0.034126984126984124, "eval/reward_1": 0.04208141416468349, "eval/reward_2": -8.863333333333353, "losses_1/value_loss": 1.2926300764083862, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 2.780929207801819e-05, "losses_1/entropy": 3.970261812210083, "losses_1/old_approx_kl": 8.857250213623047e-05, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.029732704162597656, "global_step": 32, "_wandb": {"runtime": 80}}