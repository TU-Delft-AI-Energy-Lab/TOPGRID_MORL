{"train/reward_0": 0.000992063492063492, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 4, "_timestamp": 1721657711.1956959, "_runtime": 47.287314891815186, "_step": 128, "eval/reward_0": 0.0835813492063492, "eval/reward_1": 0.12387015987964912, "eval/reward_2": -0.5, "losses_1/value_loss": 0.2442975491285324, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -2.142786979675293e-05, "losses_1/entropy": 3.970181941986084, "losses_1/old_approx_kl": -0.0005295276641845703, "losses_1/approx_kl": 2.2873282432556152e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.2904174327850342, "global_step": 128, "_wandb": {"runtime": 46}}