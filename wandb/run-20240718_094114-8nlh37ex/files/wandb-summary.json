{"train/reward_0": 2.0, "train/reward_1": 1.0435101467349672, "train/reward_2": -201.59999999999283, "train/grid2opsteps": 2016, "_timestamp": 1721288586.989891, "_runtime": 112.54331493377686, "_step": 32, "eval/reward_0": 0.15659722222222222, "eval/reward_1": 0.09584983415535077, "eval/reward_2": -18.57333333333289, "losses_1/value_loss": 4888.24658203125, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.00018265843391418457, "losses_1/entropy": 3.970215320587158, "losses_1/old_approx_kl": -0.00019294023513793945, "losses_1/approx_kl": 5.960464477539063e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.010068774223327637, "global_step": 32, "_wandb": {"runtime": 111}}