{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledDistance": 0.0, "train/reward_TopoAction": -2.0, "train/grid2opsteps": 221, "_timestamp": 1722508786.6974037, "_runtime": 367.9776186943054, "_step": 1024, "eval/reward_ScaledLinesCapacity": 0.0001932904429305555, "eval/reward_ScaledDistance": 0.0003086419753086419, "eval/reward_TopoAction": -1.0, "eval/steps": 3.0, "losses_1/value_loss": 293962.40625, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.0653902217745781, "losses_1/entropy": 3.3311893939971924, "losses_1/old_approx_kl": 0.014817841351032257, "losses_1/approx_kl": 0.015025749802589417, "losses_1/clipfrac": 0.034375, "losses_1/explained_variance": 0.010874569416046143, "global_step": 1024, "_wandb": {"runtime": 367}}