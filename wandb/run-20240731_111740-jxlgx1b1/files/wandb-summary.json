{"train/reward_0": 0.0, "train/reward_1": 0.00026814621177620007, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722417592.36257, "_runtime": 131.78567910194397, "_step": 256, "eval/reward_0": 0.00554728835978836, "eval/reward_1": 0.005843002849664401, "eval/reward_2": -0.7333333333333334, "eval/steps": 84.5, "losses_1/value_loss": 1.2320348024368286, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15938174724578857, "losses_1/entropy": 3.8491010665893555, "losses_1/old_approx_kl": 0.12348686158657074, "losses_1/approx_kl": 0.17154264450073242, "losses_1/clipfrac": 0.5895833333333333, "losses_1/explained_variance": -0.014605522155761719, "global_step": 256, "_wandb": {"runtime": 131}}