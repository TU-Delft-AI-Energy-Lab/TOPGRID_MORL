{"train/reward_0": 0.0, "train/reward_1": 0.06288554216118702, "train/reward_2": 140.0, "train/grid2opsteps": 140, "_timestamp": 1721290607.386704, "_runtime": 108.13955402374268, "_step": 32, "eval/reward_0": 0.21739417989417992, "eval/reward_1": 0.11980717233826135, "eval/reward_2": 211.4, "losses_1/value_loss": 2878.0458984375, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 0.0001311972737312317, "losses_1/entropy": 3.9702582359313965, "losses_1/old_approx_kl": -0.00014770030975341797, "losses_1/approx_kl": 1.1920928955078125e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.002843618392944336, "global_step": 32, "_wandb": {"runtime": 106}}