{"train/reward_0": 1.0, "train/reward_1": 0.9458337610164564, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721655189.809354, "_runtime": 148.64608907699585, "_step": 128, "eval/reward_0": 0.10527447089947088, "eval/reward_1": 0.09431182402836197, "eval/reward_2": -0.23333333333333334, "losses_1/value_loss": 0.1441180557012558, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.00029876455664634705, "losses_1/entropy": 3.9701194763183594, "losses_1/old_approx_kl": -0.00015294551849365234, "losses_1/approx_kl": 2.7194619178771973e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.31457823514938354, "global_step": 128, "_wandb": {"runtime": 147}}