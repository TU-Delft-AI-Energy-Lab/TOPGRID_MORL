{"train/reward_0": 0.0, "train/reward_1": 0.0003719704463953654, "train/reward_2": 0.0, "train/grid2opsteps": 230, "_timestamp": 1721657609.6894946, "_runtime": 116.77542066574097, "_step": 128, "eval/reward_0": 1.0, "eval/reward_1": 0.6843243393746817, "eval/reward_2": 0.0, "losses_1/value_loss": 0.17201952636241913, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0007992899045348167, "losses_1/entropy": 3.970205783843994, "losses_1/old_approx_kl": 0.0001928582787513733, "losses_1/approx_kl": 3.952533006668091e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.1224328875541687, "global_step": 128, "_wandb": {"runtime": 115}}