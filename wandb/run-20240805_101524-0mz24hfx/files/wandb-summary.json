{"train/reward_ScaledLinesCapacity": 0.045270496885094935, "train/reward_ScaledTopoDepth": -0.03457142857142851, "train/reward_SubstationSwitching": -0.023255813953488372, "train/grid2opsteps": 121, "_timestamp": 1722845853.3981512, "_runtime": 129.20734810829163, "_step": 302, "eval/reward_ScaledLinesCapacity": 0.0028395523867634624, "eval/reward_ScaledTopoDepth": -0.0047142857142857065, "eval/reward_SubstationSwitching": -0.25, "eval/steps": 50.5, "losses_1/value_loss": 5.385330677032471, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.1181056797504425, "losses_1/entropy": 3.7894814014434814, "losses_1/old_approx_kl": 0.03713560104370117, "losses_1/approx_kl": 0.06298790872097015, "losses_1/clipfrac": 0.25625, "losses_1/explained_variance": -10.790486335754395, "global_step": 256, "_wandb": {"runtime": 129}}