{"train/reward_0": 0.0, "train/reward_1": 0.0002639728834485954, "train/reward_2": -1.0, "train/grid2opsteps": 403, "_timestamp": 1721726510.0110173, "_runtime": 189.39367938041687, "_step": 77, "eval/reward_0": 0.11027612433862434, "eval/reward_1": 0.09213820644732337, "eval/reward_2": -0.4222222222222223, "losses_1/value_loss": 0.6819555163383484, "charts_1/learning_rate": 0.00018749999999999998, "losses_1/policy_loss": -0.00032816827297210693, "losses_1/entropy": 3.969977855682373, "losses_1/old_approx_kl": -0.0006877779960632324, "losses_1/approx_kl": 6.5267086029052734e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.6097984313964844, "global_step": 64, "_wandb": {"runtime": 192}}