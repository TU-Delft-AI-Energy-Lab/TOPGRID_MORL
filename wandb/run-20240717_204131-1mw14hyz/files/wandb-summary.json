{"train/reward_0": 0.000496031746031746, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1721241721.9348564, "_runtime": 30.33627939224243, "_step": 16, "eval/reward_0": 0.28134920634920635, "eval/reward_1": 219.66773621439933, "eval/reward_2": -30.369999999999227, "losses_1/value_loss": 0.20936541259288788, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.00015733391046524048, "losses_1/entropy": 3.9702696800231934, "losses_1/old_approx_kl": -0.00010699033737182617, "losses_1/approx_kl": 5.960464477539063e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.001861274242401123, "global_step": 16, "_wandb": {"runtime": 29}}