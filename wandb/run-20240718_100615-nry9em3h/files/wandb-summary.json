{"train/reward_0": 0.0, "train/reward_1": 0.00023762080077455194, "train/reward_2": 0.5, "train/grid2opsteps": 1, "_timestamp": 1721290067.287621, "_runtime": 92.17591595649719, "_step": 32, "eval/reward_0": 0.09327050264550264, "eval/reward_1": 0.06394829539857867, "eval/reward_2": 120.0, "losses_1/value_loss": 263.37298583984375, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -1.0244548320770264e-05, "losses_1/entropy": 3.9702720642089844, "losses_1/old_approx_kl": -1.704692840576172e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0005691051483154297, "global_step": 24, "_wandb": {"runtime": 110}}