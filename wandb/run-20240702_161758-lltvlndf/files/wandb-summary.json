{"losses_1/value_loss": 1564695.5, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.1583225578069687, "losses_1/entropy": 4.504051208496094, "losses_1/old_approx_kl": 0.014126896858215332, "losses_1/approx_kl": 0.06082630529999733, "losses_1/clipfrac": 0.9014441301425298, "losses_1/explained_variance": 0.005143105983734131, "global_step": 342, "_timestamp": 1719929975.542472, "_runtime": 96.9828929901123, "_step": 199, "charts_1/episode_reward_sum": 55.3541130253247, "charts_1/episode": 99, "charts_1/episode_reward_0": 0.0, "charts_1/episode_reward_1": 47.35411302532471, "charts_1/episode_reward_2": 7.999999999999991, "_wandb": {"runtime": 95}}