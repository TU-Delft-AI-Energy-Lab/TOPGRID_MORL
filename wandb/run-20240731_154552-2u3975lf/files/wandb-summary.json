{"train/reward_0": 0.18320445301904228, "train/reward_1": 0.0, "train/reward_2": 0.0, "train/grid2opsteps": 390, "_timestamp": 1722434101.94859, "_runtime": 549.8496191501617, "_step": 1024, "eval/reward_0": 0.6590867180658625, "eval/reward_1": 1.5, "eval/reward_2": -0.25, "eval/steps": 2016.0, "losses_1/value_loss": 0.3892945945262909, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07718312740325928, "losses_1/entropy": 3.5105955600738525, "losses_1/old_approx_kl": 0.10911351442337036, "losses_1/approx_kl": 0.04728875309228897, "losses_1/clipfrac": 0.2421875, "losses_1/explained_variance": 0.3862490653991699, "global_step": 1024, "_wandb": {"runtime": 549}}