{"train/reward_0": 0.0, "train/reward_1": 0.047822462503353924, "train/reward_2": 0.0, "train/grid2opsteps": 141, "_timestamp": 1722346249.956992, "_runtime": 343.5851209163666, "_step": 626, "eval/reward_0": 0.1175760582010582, "eval/reward_1": 0.10915840667674297, "eval/reward_2": -0.48888888888888893, "losses_1/value_loss": 0.3363647758960724, "charts_1/learning_rate": 0.00018749999999999998, "losses_1/policy_loss": -0.0035561397671699524, "losses_1/entropy": 3.9675965309143066, "losses_1/old_approx_kl": 0.0005877912044525146, "losses_1/approx_kl": 0.00011060014367103577, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.2995411157608032, "global_step": 512, "_wandb": {"runtime": 344}}