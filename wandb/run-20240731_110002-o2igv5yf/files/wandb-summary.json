{"train/reward_0": 0.0, "train/reward_1": 900.2718168124557, "train/reward_2": 0.0, "train/grid2opsteps": 1273, "_timestamp": 1722416426.6977386, "_runtime": 24.120524644851685, "_step": 64, "eval/reward_0": 0.5031179138321995, "eval/reward_1": 0.0, "eval/reward_2": 0.0, "eval/steps": 1052.0, "losses_1/value_loss": 168255.65625, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.15863029658794403, "losses_1/entropy": 3.7766151428222656, "losses_1/old_approx_kl": -0.042427271604537964, "losses_1/approx_kl": 0.22711633145809174, "losses_1/clipfrac": 0.525, "losses_1/explained_variance": 0.007103145122528076, "global_step": 64, "_wandb": {"runtime": 23}}