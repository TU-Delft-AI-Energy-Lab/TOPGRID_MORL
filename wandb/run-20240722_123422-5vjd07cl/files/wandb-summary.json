{"train/reward_0": 0.0, "train/reward_1": 0.161406850125477, "train/reward_2": 0.0, "train/grid2opsteps": 380, "_timestamp": 1721644500.3169293, "_runtime": 37.428163290023804, "_step": 16, "eval/reward_0": 0.272296626984127, "eval/reward_1": 0.2429338484576098, "eval/reward_2": -0.13333333333333333, "losses_1/value_loss": 0.06125405803322792, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 6.563588976860046e-05, "losses_1/entropy": 3.9702582359313965, "losses_1/old_approx_kl": -0.000653386116027832, "losses_1/approx_kl": 1.7881393432617188e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.16566014289855957, "global_step": 16, "_wandb": {"runtime": 36}}