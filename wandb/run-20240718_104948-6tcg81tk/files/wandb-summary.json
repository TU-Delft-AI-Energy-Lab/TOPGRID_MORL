{"train/reward_0": 0.0, "train/reward_1": 0.00022093577555126856, "train/reward_2": -40.0, "train/grid2opsteps": 1, "_timestamp": 1721292667.602204, "_runtime": 79.39479112625122, "_step": 23, "eval/reward_0": 0.07388392857142856, "eval/reward_1": 0.08280998306358399, "eval/reward_2": -Infinity, "losses_1/value_loss": 1073.3995361328125, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -3.108382225036621e-05, "losses_1/entropy": 3.9702703952789307, "losses_1/old_approx_kl": 0.0003148317337036133, "losses_1/approx_kl": 4.470348358154297e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.001372694969177246, "global_step": 16}