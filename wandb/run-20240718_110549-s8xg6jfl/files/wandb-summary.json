{"train/reward_0": 0.0, "train/reward_1": 0.00024667650527150886, "train/reward_2": -50.0, "train/grid2opsteps": 3, "_timestamp": 1721293664.558006, "_runtime": 115.33315300941467, "_step": 24, "eval/reward_0": 0.18625165343915343, "eval/reward_1": 0.18013378476072653, "eval/reward_2": -0.1, "losses_1/value_loss": 241.5300750732422, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -0.000311143696308136, "losses_1/entropy": 3.9702720642089844, "losses_1/old_approx_kl": -0.0001919865608215332, "losses_1/approx_kl": 8.940696716308594e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.009426236152648926, "global_step": 16, "_wandb": {"runtime": 144}}