{"train/reward_0": 1.0, "train/reward_1": 0.8877596149495828, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1721823212.1665819, "_runtime": 14834.251463890076, "_step": 983, "eval/reward_0": 0.1445353835978836, "eval/reward_1": 0.11853503820900922, "eval/reward_2": -0.6277777777777779, "losses_1/value_loss": 0.4227685034275055, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.0032668262720108032, "losses_1/entropy": 3.9631311893463135, "losses_1/old_approx_kl": 0.0006599575281143188, "losses_1/approx_kl": 3.6774203181266785e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.4965968132019043, "global_step": 896, "_wandb": {"runtime": 14834}}