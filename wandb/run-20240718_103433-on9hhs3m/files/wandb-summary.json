{"train/reward_0": 2.0, "train/reward_1": 1512.6860395669937, "train/reward_2": 0.0, "train/grid2opsteps": 2022, "_timestamp": 1721291783.6696455, "_runtime": 110.61383843421936, "_step": 32, "eval/reward_0": 0.1556216931216931, "eval/reward_1": 119.2851138236622, "eval/reward_2": -30.1, "losses_1/value_loss": 162184.046875, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 1.360476016998291e-05, "losses_1/entropy": 3.970257520675659, "losses_1/old_approx_kl": 2.968311309814453e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -1.2636184692382812e-05, "global_step": 32, "_wandb": {"runtime": 109}}