{"losses_1/value_loss": 0.009428338147699833, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.11129017919301987, "losses_1/entropy": 4.5949177742004395, "losses_1/old_approx_kl": 0.009143988601863384, "losses_1/approx_kl": 0.00886768102645874, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.9435765780508518, "global_step": 3, "_timestamp": 1719924299.8685608, "_runtime": 0.8902268409729004, "_step": 1, "charts_1/episode_reward_sum": -0.6332639279819671, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.001488095238095238, "charts_1/episode_reward_1": 0.16524797677993774, "charts_1/episode_reward_2": -0.8, "_wandb": {"runtime": 0}}