{"train/reward_ScaledLinesCapacity": 9.196364618619539e-05, "train/reward_ScaledDistance": 0.00047619047619047624, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 88, "_timestamp": 1722504366.2011683, "_runtime": 34.73222041130066, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.43738077093536437, "eval/reward_ScaledDistance": 0.48988095238096807, "eval/reward_TopoAction": -0.25, "eval/steps": 1050.0, "losses_1/value_loss": 1.8305424451828003, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.12795236706733704, "losses_1/entropy": 3.823781728744507, "losses_1/old_approx_kl": 0.08546870201826096, "losses_1/approx_kl": 0.06617201119661331, "losses_1/clipfrac": 0.30555556118488314, "losses_1/explained_variance": 0.33854997158050537, "global_step": 108, "_wandb": {"runtime": 34}}