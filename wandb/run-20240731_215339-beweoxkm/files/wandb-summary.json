{"train/reward_ScaledLinesCapacity": 3.394450040763954e-05, "train/reward_Distance": 0.7142857142857143, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 96, "_timestamp": 1722455713.2705708, "_runtime": 94.12135076522827, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.4459945135073149, "eval/reward_Distance": 816.3015873016142, "eval/reward_TopoAction": -0.5, "eval/steps": 1009.5, "losses_1/value_loss": 88900.453125, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.06432885676622391, "losses_1/entropy": 3.807440757751465, "losses_1/old_approx_kl": 0.15533442795276642, "losses_1/approx_kl": 0.026703136041760445, "losses_1/clipfrac": 0.1444444466382265, "losses_1/explained_variance": 0.002844512462615967, "global_step": 108, "_wandb": {"runtime": 93}}