{"train/reward_0": 0.0, "train/reward_1": 0.7437199183872768, "train/reward_2": -0.1, "train/grid2opsteps": 15, "_timestamp": 1721287176.4510176, "_runtime": 109.20784068107605, "_step": 32, "eval/reward_0": 0.16815476190476192, "eval/reward_1": 143.03465014249088, "eval/reward_2": -21.97333333333302, "losses_1/value_loss": 91960.9375, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 0.00022548437118530273, "losses_1/entropy": 3.970256805419922, "losses_1/old_approx_kl": 0.00025969743728637695, "losses_1/approx_kl": 1.043081283569336e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0009073019027709961, "global_step": 32, "_wandb": {"runtime": 108}}