{"train/reward_0": 1.0, "train/reward_1": 0.8921930334970968, "train/reward_2": 0.0, "train/grid2opsteps": 2016, "_timestamp": 1722411336.2589178, "_runtime": 102.83268284797668, "_step": 64, "eval/reward_0": 0.12512400793650794, "eval/reward_1": 0.11536094160724936, "eval/reward_2": -0.5, "losses_1/value_loss": 0.26014843583106995, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.0033148005604743958, "losses_1/entropy": 3.9696431159973145, "losses_1/old_approx_kl": 0.0010915100574493408, "losses_1/approx_kl": 9.134411811828613e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.03851330280303955, "global_step": 64, "_wandb": {"runtime": 101}}