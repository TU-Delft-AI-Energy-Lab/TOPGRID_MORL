{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_TopoDepth": 0.0, "train/reward_SubstationSwitching": 0.0, "train/grid2opsteps": 2, "_timestamp": 1722587453.4140573, "_runtime": 160.99214816093445, "_step": 448, "eval/reward_ScaledLinesCapacity": 0.005249424318987985, "eval/reward_TopoDepth": 0.0, "eval/reward_SubstationSwitching": -0.25, "eval/steps": 42.5, "losses_1/value_loss": 2121.575927734375, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.05921969190239906, "losses_1/entropy": 3.7621469497680664, "losses_1/old_approx_kl": 0.01307147741317749, "losses_1/approx_kl": 0.016016356647014618, "losses_1/clipfrac": 0.071875, "losses_1/explained_variance": 0.0075394511222839355, "global_step": 384, "_wandb": {"runtime": 162}}