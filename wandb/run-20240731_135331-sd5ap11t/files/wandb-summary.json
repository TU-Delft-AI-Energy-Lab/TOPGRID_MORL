{"train/reward_0": 0.0, "train/reward_1": 0.0003700669753238229, "train/reward_2": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722427460.429481, "_runtime": 648.8111801147461, "_step": 1024, "eval/reward_0": 0.5833333333333334, "eval/reward_1": 0.490861968278756, "eval/reward_2": -0.3333333333333333, "eval/steps": 2016.0, "losses_1/value_loss": 0.5518050789833069, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.08060815930366516, "losses_1/entropy": 3.515166997909546, "losses_1/old_approx_kl": 0.05214868485927582, "losses_1/approx_kl": 0.042149756103754044, "losses_1/clipfrac": 0.2140625, "losses_1/explained_variance": -0.20260822772979736, "global_step": 1024, "_wandb": {"runtime": 648}}