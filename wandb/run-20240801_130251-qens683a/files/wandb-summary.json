{"train/reward_ScaledLinesCapacity": 0.4494103522934297, "train/reward_ScaledDistance": 0.4335449735449624, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 964, "_timestamp": 1722510588.8431349, "_runtime": 417.01054191589355, "_step": 704, "eval/reward_ScaledLinesCapacity": 0.602282570722105, "eval/reward_ScaledDistance": 0.5871781305114685, "eval/reward_TopoAction": -1.0, "eval/steps": 2016.0, "losses_1/value_loss": 0.773284912109375, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07240650057792664, "losses_1/entropy": 3.2942678928375244, "losses_1/old_approx_kl": 0.04395899176597595, "losses_1/approx_kl": 0.04934979975223541, "losses_1/clipfrac": 0.25625, "losses_1/explained_variance": 0.5419422090053558, "global_step": 640, "_wandb": {"runtime": 417}}