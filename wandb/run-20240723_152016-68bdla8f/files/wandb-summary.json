{"train/reward_0": 0.11433531746031746, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 461, "_timestamp": 1721740976.274267, "_runtime": 159.3906910419464, "_step": 128, "eval/reward_0": 0.20438161375661376, "eval/reward_1": 0.17329019885513647, "eval/reward_2": -0.5277777777777778, "losses_1/value_loss": 0.568466305732727, "charts_1/learning_rate": 3.75e-05, "losses_1/policy_loss": 0.00014566956087946892, "losses_1/entropy": 3.970081329345703, "losses_1/old_approx_kl": -0.00018647313117980957, "losses_1/approx_kl": 4.470348358154297e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.18935507535934448, "global_step": 128, "_wandb": {"runtime": 158}}