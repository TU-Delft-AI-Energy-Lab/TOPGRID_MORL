{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledTopoDepth": 0.0, "train/reward_SubstationSwitching": 0.0, "train/grid2opsteps": 796, "_timestamp": 1722845959.795004, "_runtime": 82.85447478294373, "_step": 204, "eval/reward_ScaledLinesCapacity": 0.006109584104393703, "eval/reward_ScaledTopoDepth": -0.00567857142857143, "eval/reward_SubstationSwitching": -0.375, "eval/steps": 32.0, "losses_1/value_loss": 1.1472169160842896, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.12902574241161346, "losses_1/entropy": 3.8835129737854004, "losses_1/old_approx_kl": 0.022680379450321198, "losses_1/approx_kl": 0.046438731253147125, "losses_1/clipfrac": 0.23125, "losses_1/explained_variance": -2.8611600399017334, "global_step": 192, "_wandb": {"runtime": 84}}