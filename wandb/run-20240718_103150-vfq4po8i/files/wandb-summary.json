{"train/reward_0": 0.0, "train/reward_1": 0.3764604926109314, "train/reward_2": -40.0, "train/grid2opsteps": 3, "_timestamp": 1721291657.3653424, "_runtime": 147.2320113182068, "_step": 32, "eval/reward_0": 0.41433531746031743, "eval/reward_1": 318.45397122552, "eval/reward_2": -15.533333333333333, "losses_1/value_loss": 1912.9320068359375, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 6.556510925292969e-05, "losses_1/entropy": 3.970264434814453, "losses_1/old_approx_kl": -8.040666580200195e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0117989182472229, "global_step": 32, "_wandb": {"runtime": 146}}