{"train/reward_0": 2.0, "train/reward_1": 2017.4658922512967, "train/reward_2": -201.59999999999283, "train/grid2opsteps": 2016, "_timestamp": 1721220164.1719768, "_runtime": 74.29406189918518, "_step": 16, "eval/reward_0": 0.2427579365079365, "eval/reward_1": 6.88677567316469, "eval/reward_2": -29.019999999999424, "losses_1/value_loss": 295168.0625, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 4.118680953979492e-05, "losses_1/entropy": 3.9702744483947754, "losses_1/old_approx_kl": -2.4497509002685547e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -5.269050598144531e-05, "global_step": 16, "_wandb": {"runtime": 73}}