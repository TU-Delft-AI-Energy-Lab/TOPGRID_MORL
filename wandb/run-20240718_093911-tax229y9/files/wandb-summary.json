{"train/reward_0": 0.0, "train/reward_1": 9.20000944877429e-05, "train/reward_2": -0.1, "train/grid2opsteps": 7638, "_timestamp": 1721288461.9412773, "_runtime": 110.82132935523987, "_step": 32, "eval/reward_0": 0.21759259259259262, "eval/reward_1": 0.11255969782003354, "eval/reward_2": -25.20333333333264, "losses_1/value_loss": 680.748291015625, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -1.7195940017700195e-05, "losses_1/entropy": 3.9702255725860596, "losses_1/old_approx_kl": 0.00011980533599853516, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.02680182456970215, "global_step": 32, "_wandb": {"runtime": 109}}