{"train/reward_0": 0.0003573763409954634, "train/reward_1": 0.6666666666666667, "train/reward_2": 0.0, "train/grid2opsteps": 446, "_timestamp": 1722434902.1817462, "_runtime": 55.0395143032074, "_step": 136, "eval/reward_0": 0.002683649561072424, "eval/reward_1": 9.67857142857142, "eval/reward_2": -0.875, "eval/steps": 44.0, "losses_1/value_loss": 115656.3984375, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.07682378590106964, "losses_1/entropy": 3.9285316467285156, "losses_1/old_approx_kl": 0.07042241841554642, "losses_1/approx_kl": 0.0395708903670311, "losses_1/clipfrac": 0.3125, "losses_1/explained_variance": 0.0057579874992370605, "global_step": 128, "_wandb": {"runtime": 54}}