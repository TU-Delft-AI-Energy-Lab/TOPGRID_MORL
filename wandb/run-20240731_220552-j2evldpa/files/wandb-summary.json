{"train/reward_ScaledLinesCapacity": 0.027643374897128484, "train/reward_ScaledDistance": 0.033809523809523845, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 71, "_timestamp": 1722456502.1185706, "_runtime": 150.0059084892273, "_step": 107, "eval/reward_ScaledLinesCapacity": 0.19873745719338676, "eval/reward_ScaledDistance": 0.2531944444444513, "eval/reward_TopoAction": -0.25, "eval/steps": 1050.0, "losses_1/value_loss": 4.104068279266357, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.12268083542585373, "losses_1/entropy": 3.9056222438812256, "losses_1/old_approx_kl": 0.06559651345014572, "losses_1/approx_kl": 0.05769321694970131, "losses_1/clipfrac": 0.3000000029802322, "losses_1/explained_variance": -5.027896881103516, "global_step": 72, "_wandb": {"runtime": 152}}