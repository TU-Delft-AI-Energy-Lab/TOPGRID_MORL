{"train/reward_ScaledLinesCapacity": 0.06915717473840144, "train/reward_ScaledDistance": -31.857142857142758, "train/reward_TopoAction": 0.0, "train/grid2opsteps": 223, "_timestamp": 1722456302.3268871, "_runtime": 131.46269822120667, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.00269621023462118, "eval/reward_ScaledDistance": -1.0714285714285703, "eval/reward_TopoAction": -0.75, "eval/steps": 40.5, "losses_1/value_loss": 13346.283203125, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.06601949781179428, "losses_1/entropy": 3.944406270980835, "losses_1/old_approx_kl": 0.0838891938328743, "losses_1/approx_kl": 0.014437645673751831, "losses_1/clipfrac": 0.05000000149011612, "losses_1/explained_variance": 0.010706603527069092, "global_step": 72, "_wandb": {"runtime": 139}}