{"train/reward_0": 0.0, "train/reward_1": 0.03496168925595713, "train/reward_2": 108.0, "train/grid2opsteps": 108, "_timestamp": 1721290729.5445058, "_runtime": 106.65007972717285, "_step": 32, "eval/reward_0": 0.14735449735449738, "eval/reward_1": 0.09151655713623358, "eval/reward_2": 160.0, "losses_1/value_loss": 446846.96875, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -1.430511474609375e-06, "losses_1/entropy": 3.970245361328125, "losses_1/old_approx_kl": 4.410743713378906e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0004521608352661133, "global_step": 32, "_wandb": {"runtime": 105}}