{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledDistance": 0.0, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 5, "_timestamp": 1722507954.1545756, "_runtime": 302.74759554862976, "_step": 789, "eval/reward_ScaledLinesCapacity": 0.004022897302357958, "eval/reward_ScaledDistance": 0.004764550264550255, "eval/reward_TopoAction": -0.8, "eval/steps": 61.0, "losses_1/value_loss": 1.8972305059432983, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.06996352970600128, "losses_1/entropy": 3.5329995155334473, "losses_1/old_approx_kl": 0.10455546528100967, "losses_1/approx_kl": 0.02689095214009285, "losses_1/clipfrac": 0.159375, "losses_1/explained_variance": 0.5217449963092804, "global_step": 768, "_wandb": {"runtime": 302}}