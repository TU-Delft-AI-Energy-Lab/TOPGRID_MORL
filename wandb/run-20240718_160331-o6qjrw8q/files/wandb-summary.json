{"train/reward_0": 0.034970238095238096, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 397, "_timestamp": 1721311473.4050784, "_runtime": 62.1637864112854, "_step": 24, "eval/reward_0": 0.26740244708994704, "eval/reward_1": 0.2266621072526084, "eval/reward_2": -0.2, "losses_1/value_loss": 0.2724781334400177, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": 0.0018006116151809692, "losses_1/entropy": 3.9702467918395996, "losses_1/old_approx_kl": -0.001540541648864746, "losses_1/approx_kl": 8.374452590942383e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -2.021691083908081, "global_step": 16, "_wandb": {"runtime": 63}}