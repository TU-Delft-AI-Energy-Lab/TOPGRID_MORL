{"train/reward_0": 0.0, "train/reward_1": 0.00011467142624616704, "train/reward_2": -50.0, "train/grid2opsteps": 3, "_timestamp": 1721292350.2423503, "_runtime": 141.49600434303284, "_step": 32, "eval/reward_0": 0.05209986772486772, "eval/reward_1": 0.06882479849318703, "eval/reward_2": -Infinity, "losses_1/value_loss": 1738.70703125, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 2.6971101760864258e-05, "losses_1/entropy": 3.97025990486145, "losses_1/old_approx_kl": -0.00022226572036743164, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0025862455368041992, "global_step": 32, "_wandb": {"runtime": 140}}