{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledTopoDepth": 0.0, "train/reward_SubstationSwitching": 0.0, "train/grid2opsteps": 1, "_timestamp": 1722845656.3046064, "_runtime": 309.2613673210144, "_step": 1024, "eval/reward_ScaledLinesCapacity": 6.96158566129608e-05, "eval/reward_ScaledTopoDepth": -7.936507936507935e-05, "eval/reward_SubstationSwitching": -0.16666666666666666, "eval/steps": 2.0, "losses_1/value_loss": 2.330751895904541, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.1007460504770279, "losses_1/entropy": 3.3488857746124268, "losses_1/old_approx_kl": -0.007048159837722778, "losses_1/approx_kl": 0.027599064633250237, "losses_1/clipfrac": 0.19375, "losses_1/explained_variance": -0.12818682193756104, "global_step": 1024, "_wandb": {"runtime": 308}}