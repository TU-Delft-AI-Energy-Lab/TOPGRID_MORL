{"train/reward_0": 0.0, "train/reward_1": 0.00013773223957389658, "train/reward_2": 0.6, "train/grid2opsteps": 157, "_timestamp": 1721289793.5168822, "_runtime": 122.29315423965454, "_step": 32, "eval/reward_0": 0.08893849206349207, "eval/reward_1": 0.05362773151920286, "eval/reward_2": 111.35333333333334, "losses_1/value_loss": 169287.046875, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 5.000084638595581e-05, "losses_1/entropy": 3.9702625274658203, "losses_1/old_approx_kl": -0.00011581182479858398, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0006462335586547852, "global_step": 32, "_wandb": {"runtime": 121}}