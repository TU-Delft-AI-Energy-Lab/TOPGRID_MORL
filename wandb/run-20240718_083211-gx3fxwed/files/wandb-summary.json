{"train/reward_0": 0.0, "train/reward_1": 5.7207891047000885, "train/reward_2": -0.7999999999999999, "train/grid2opsteps": 262, "_timestamp": 1721284408.6057506, "_runtime": 77.06916046142578, "_step": 16, "eval/reward_0": 0.20014880952380956, "eval/reward_1": 154.1241752102971, "eval/reward_2": -21.60666666666616, "losses_1/value_loss": 81130.1640625, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 2.053380012512207e-05, "losses_1/entropy": 3.970271587371826, "losses_1/old_approx_kl": 3.439188003540039e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.00060272216796875, "global_step": 16, "_wandb": {"runtime": 63}}