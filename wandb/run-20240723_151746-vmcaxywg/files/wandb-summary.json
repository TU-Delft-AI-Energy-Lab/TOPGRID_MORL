{"train/reward_0": 0.000992063492063492, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 4, "_timestamp": 1721740790.9607916, "_runtime": 124.07265758514404, "_step": 96, "eval/reward_0": 0.1832010582010582, "eval/reward_1": 0.16784074744713315, "eval/reward_2": -0.6, "losses_1/value_loss": 0.7469360828399658, "charts_1/learning_rate": 0.0001125, "losses_1/policy_loss": 0.0002447068691253662, "losses_1/entropy": 3.9700100421905518, "losses_1/old_approx_kl": 0.0006475746631622314, "losses_1/approx_kl": 7.674098014831543e-07, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.260961651802063, "global_step": 96, "_wandb": {"runtime": 122}}