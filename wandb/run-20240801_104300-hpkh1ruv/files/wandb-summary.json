{"train/reward_ScaledLinesCapacity": 0.00034934883600970686, "train/reward_ScaledDistance": 0.0004497354497354497, "train/reward_TopoAction": -1.0, "train/grid2opsteps": 1, "_timestamp": 1722501827.3984053, "_runtime": 47.27780342102051, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.32412034478860585, "eval/reward_ScaledDistance": 0.4592548500881885, "eval/reward_TopoAction": -0.3333333333333333, "eval/steps": 1048.0, "losses_1/value_loss": 2.9040026664733887, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.10649451613426208, "losses_1/entropy": 3.842726707458496, "losses_1/old_approx_kl": 0.16393929719924927, "losses_1/approx_kl": 0.04721985012292862, "losses_1/clipfrac": 0.23333333469927311, "losses_1/explained_variance": -1.873063087463379, "global_step": 108, "_wandb": {"runtime": 46}}