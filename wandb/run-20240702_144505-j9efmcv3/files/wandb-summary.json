{"losses_1/value_loss": 0.4384555518627167, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.13218382000923157, "losses_1/entropy": 4.5945940017700195, "losses_1/old_approx_kl": -0.06407757103443146, "losses_1/approx_kl": 0.012842357158660889, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.11061203479766846, "global_step": 5, "_timestamp": 1719924307.1011024, "_runtime": 1.881117343902588, "_step": 1, "charts_1/episode_reward_sum": -3.614863035413954, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.00248015873015873, "charts_1/episode_reward_1": 0.28265680585588726, "charts_1/episode_reward_2": -3.9, "_wandb": {"runtime": 0}}