{"train/reward_0": 0.0, "train/reward_1": 0.24937903881072998, "train/reward_2": -60.0, "train/grid2opsteps": 1, "_timestamp": 1721291925.8355372, "_runtime": 126.53661823272705, "_step": 32, "eval/reward_0": 0.3419146825396826, "eval/reward_1": 254.489872074411, "eval/reward_2": -24.466666666666665, "losses_1/value_loss": 122571.3671875, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.00017796456813812256, "losses_1/entropy": 3.9702582359313965, "losses_1/old_approx_kl": -0.00014853477478027344, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 7.683038711547852e-05, "global_step": 32, "_wandb": {"runtime": 125}}