{"train/reward_0": 0.0, "train/reward_1": 0.00029081056948297886, "train/reward_2": -1.0, "train/grid2opsteps": 146, "_timestamp": 1722409982.2827065, "_runtime": 106.47498154640198, "_step": 64, "losses_1/value_loss": 0.7053808569908142, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": -0.00242595374584198, "losses_1/entropy": 3.9699811935424805, "losses_1/old_approx_kl": 0.0005446076393127441, "losses_1/approx_kl": 4.462897777557373e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.633573055267334, "global_step": 64, "_wandb": {"runtime": 106}}