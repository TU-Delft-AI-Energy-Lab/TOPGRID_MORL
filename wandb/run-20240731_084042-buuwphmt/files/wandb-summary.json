{"train/reward_0": 0.0, "train/reward_1": 0.0002049273600425389, "train/reward_2": -0.8333333333333334, "train/grid2opsteps": 1, "_timestamp": 1722408103.8302295, "_runtime": 61.32651448249817, "_step": 48, "losses_1/value_loss": 0.2860244810581207, "charts_1/learning_rate": 0.000225, "losses_1/policy_loss": -0.009640375152230263, "losses_1/entropy": 3.9701173305511475, "losses_1/old_approx_kl": -0.004475921392440796, "losses_1/approx_kl": 7.396936416625977e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.22371184825897217, "global_step": 32, "_wandb": {"runtime": 62}}