{"train/reward_0": 0.0, "train/reward_1": 0.00014516228789202823, "train/reward_2": 0.0, "train/grid2opsteps": 3, "_timestamp": 1721370457.9204102, "_runtime": 66.99926424026489, "_step": 16, "eval/reward_0": 0.12137896825396825, "eval/reward_1": 0.13625491371323253, "eval/reward_2": -0.2, "losses_1/value_loss": 0.11846229434013367, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 3.471970558166504e-05, "losses_1/entropy": 3.970264196395874, "losses_1/old_approx_kl": -7.861852645874023e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.03981578350067139, "global_step": 16, "_wandb": {"runtime": 65}}