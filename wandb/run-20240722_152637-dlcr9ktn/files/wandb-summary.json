{"train/reward_0": 0.0, "train/reward_1": 0.0002288775877631173, "train/reward_2": 0.0, "train/grid2opsteps": 2, "_timestamp": 1721654911.9531212, "_runtime": 114.09606909751892, "_step": 32, "eval/reward_0": 0.08179563492063492, "eval/reward_1": 0.07830138603618143, "eval/reward_2": -0.3, "losses_1/value_loss": 0.01882922276854515, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 2.9340386390686035e-05, "losses_1/entropy": 3.9702372550964355, "losses_1/old_approx_kl": 8.720159530639648e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.5560546517372131, "global_step": 32, "_wandb": {"runtime": 112}}