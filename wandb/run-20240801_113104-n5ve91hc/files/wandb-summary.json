{"train/reward_ScaledLinesCapacity": 0.0, "train/reward_ScaledDistance": 0.0, "train/reward_TopoAction": -90.0, "train/grid2opsteps": 90, "_timestamp": 1722504708.856837, "_runtime": 44.50624895095825, "_step": 108, "eval/reward_ScaledLinesCapacity": 0.2400961790594635, "eval/reward_ScaledDistance": 0.23999338624339409, "eval/reward_TopoAction": -0.75, "eval/steps": 1008.5, "losses_1/value_loss": 34383.48046875, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.11306638270616531, "losses_1/entropy": 3.7702534198760986, "losses_1/old_approx_kl": 0.27557146549224854, "losses_1/approx_kl": 0.0768977552652359, "losses_1/clipfrac": 0.2222222253680229, "losses_1/explained_variance": -0.0023050308227539062, "global_step": 108, "_wandb": {"runtime": 44}}