{"train/reward_0": 0.0, "train/reward_1": 0.00023082003283359765, "train/reward_2": 0.0, "train/grid2opsteps": 3, "_timestamp": 1721644606.6486862, "_runtime": 23.518651247024536, "_step": 16, "eval/reward_0": 0.14596560846560847, "eval/reward_1": 0.12540230534473595, "eval/reward_2": -0.2, "losses_1/value_loss": 0.37961292266845703, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.00035989657044410706, "losses_1/entropy": 3.970262050628662, "losses_1/old_approx_kl": -7.992982864379883e-05, "losses_1/approx_kl": 8.940696716308594e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -1.669069766998291, "global_step": 16, "_wandb": {"runtime": 22}}