{"train/reward_0": 0.0, "train/reward_1": 0.0001762728892814047, "train/reward_2": 0.6, "train/grid2opsteps": 1, "_timestamp": 1721289960.615327, "_runtime": 147.60115385055542, "_step": 32, "eval/reward_0": 0.21421957671957673, "eval/reward_1": 0.1198783368338737, "eval/reward_2": 229.54666666666665, "losses_1/value_loss": 930.728515625, "charts_1/learning_rate": 7.5e-05, "losses_1/policy_loss": 3.9830803871154785e-05, "losses_1/entropy": 3.970247983932495, "losses_1/old_approx_kl": 9.59634780883789e-05, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.011381089687347412, "global_step": 32, "_wandb": {"runtime": 146}}