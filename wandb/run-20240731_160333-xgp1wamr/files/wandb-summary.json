{"train/reward_0": 0.004089867557011448, "train/reward_1": 10.600000000000001, "train/reward_2": 0.0, "train/grid2opsteps": 459, "_timestamp": 1722434797.033578, "_runtime": 183.4074559211731, "_step": 443, "eval/reward_0": 0.4798791205817196, "eval/reward_1": 1008.0, "eval/reward_2": -0.5, "eval/steps": 1008.5, "losses_1/value_loss": 183850.484375, "charts_1/learning_rate": 0.0005, "losses_1/policy_loss": -0.05301210656762123, "losses_1/entropy": 3.7317562103271484, "losses_1/old_approx_kl": 0.041782692074775696, "losses_1/approx_kl": 0.02594788745045662, "losses_1/clipfrac": 0.18125, "losses_1/explained_variance": 0.008726894855499268, "global_step": 384, "_wandb": {"runtime": 183}}