{"train/reward_0": 0.001488095238095238, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 6, "_timestamp": 1721286139.886486, "_runtime": 68.63779211044312, "_step": 16, "eval/reward_0": 0.2815310846560847, "eval/reward_1": 233.3429232309262, "eval/reward_2": -30.086666666665785, "losses_1/value_loss": 100543.78125, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 4.2766332626342773e-05, "losses_1/entropy": 3.9702675342559814, "losses_1/old_approx_kl": 0.00010514259338378906, "losses_1/approx_kl": 0.0, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0010867118835449219, "global_step": 16, "_wandb": {"runtime": 67}}