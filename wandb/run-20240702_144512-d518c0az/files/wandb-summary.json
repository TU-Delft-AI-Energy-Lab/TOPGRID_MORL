{"losses_1/value_loss": 1791.31787109375, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.0857727900147438, "losses_1/entropy": 4.59490966796875, "losses_1/old_approx_kl": 0.03403883054852486, "losses_1/approx_kl": 0.0048983218148350716, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0023421645164489746, "global_step": 5, "_timestamp": 1719924314.655157, "_runtime": 2.2651381492614746, "_step": 1, "charts_1/episode_reward_sum": 268.1411865677155, "charts_1/episode": 0, "charts_1/episode_reward_0": 0.0, "charts_1/episode_reward_1": 229.24118656771523, "charts_1/episode_reward_2": 38.90000000000028, "_wandb": {"runtime": 1}}