{"train/reward_0": 0.0, "train/reward_1": 49.39000189304352, "train/reward_2": -10.19999999999998, "train/grid2opsteps": 102, "_timestamp": 1721204047.7339807, "_runtime": 50.754231691360474, "_step": 16, "eval/reward_0": 0.08215939153439153, "eval/reward_1": 64.63033009256638, "eval/reward_2": -11.286666666666614, "losses_1/value_loss": 0.35855698585510254, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.001319587230682373, "losses_1/entropy": 3.9702672958374023, "losses_1/old_approx_kl": -0.0009289383888244629, "losses_1/approx_kl": 2.115964889526367e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0007538795471191406, "global_step": 16, "_wandb": {"runtime": 49}}