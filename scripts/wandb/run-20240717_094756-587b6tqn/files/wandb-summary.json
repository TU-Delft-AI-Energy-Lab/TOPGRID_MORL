{"train/reward_0": 0.0, "train/reward_1": 0.42450058460235596, "train/reward_2": -0.1, "train/grid2opsteps": 743, "_timestamp": 1721202886.545731, "_runtime": 409.88782000541687, "_step": 64, "eval/reward_0": 0.10750661375661377, "eval/reward_1": 109.70097892905275, "eval/reward_2": -14.89666666666645, "losses_1/value_loss": 55392.828125, "charts_1/learning_rate": 0.0002625, "losses_1/policy_loss": -0.020375065505504608, "losses_1/entropy": 3.969865083694458, "losses_1/old_approx_kl": 0.013585031032562256, "losses_1/approx_kl": 0.00047954171895980835, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0017347931861877441, "global_step": 48, "_wandb": {"runtime": 420}}