{"train/reward_0": 0.0, "train/reward_1": 0.3923761248588562, "train/reward_2": -0.1, "train/grid2opsteps": 1, "_timestamp": 1721201023.281347, "_runtime": 201.9733021259308, "_step": 32, "eval/reward_0": 0.16957671957671958, "eval/reward_1": 139.79413383876283, "eval/reward_2": -19.509999999999657, "losses_1/value_loss": 182890.546875, "charts_1/learning_rate": 0.0003, "losses_1/policy_loss": -0.04251734912395477, "losses_1/entropy": 3.9700636863708496, "losses_1/old_approx_kl": -0.010671854019165039, "losses_1/approx_kl": 0.0014003589749336243, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.00019180774688720703, "global_step": 16, "_wandb": {"runtime": 209}}