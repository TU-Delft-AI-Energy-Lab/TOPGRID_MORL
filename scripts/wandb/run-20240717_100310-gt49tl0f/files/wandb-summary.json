{"train/reward_0": 0.35466269841269843, "train/reward_1": 0.0, "train/reward_2": -1.0, "train/grid2opsteps": 2854, "_timestamp": 1721203682.5295227, "_runtime": 291.68844270706177, "_step": 47, "eval/reward_0": 0.2765046296296296, "eval/reward_1": 184.0728006315728, "eval/reward_2": -25.99333333333259, "losses_1/value_loss": 98578.875, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.0002724621444940567, "losses_1/entropy": 3.969933032989502, "losses_1/old_approx_kl": 2.321600914001465e-05, "losses_1/approx_kl": 4.082918167114258e-06, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0005384087562561035, "global_step": 32, "DoNothing/train/reward_0": 0.04712301587301587, "DoNothing/train/reward_1": 0.0, "DoNothing/train/reward_2": -1.0, "DoNothing/train/grid2opsteps": 1489, "_wandb": {"runtime": 290}, "DoNothing/eval/reward_0": 0.002742763772175537, "DoNothing/eval/reward_1": 2.3202971109823016, "DoNothing/eval/reward_2": -0.6058823529411754}