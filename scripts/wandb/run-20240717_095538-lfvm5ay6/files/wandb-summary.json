{"train/reward_0": 0.0, "train/reward_1": 0.6591940224170685, "train/reward_2": -0.1, "train/grid2opsteps": 297, "_timestamp": 1721203159.0136054, "_runtime": 220.03150725364685, "_step": 32, "eval/reward_0": 0.22617394179894182, "eval/reward_1": 173.5385583342186, "eval/reward_2": -24.183333333332733, "losses_1/value_loss": 50663.03125, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": -0.006372883915901184, "losses_1/entropy": 3.96979022026062, "losses_1/old_approx_kl": 0.00890687108039856, "losses_1/approx_kl": 7.792562246322632e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0005141496658325195, "global_step": 32, "_wandb": {"runtime": 219}}