{"train/reward_0": 0.0, "train/reward_1": 0.41738319396972656, "train/reward_2": -0.1, "train/grid2opsteps": 1, "_timestamp": 1721200559.4707928, "_runtime": 798.5564737319946, "_step": 128, "eval/reward_0": 0.10203373015873014, "eval/reward_1": 97.55339169576764, "eval/reward_2": -14.289999999999804, "losses_1/value_loss": 196157.25, "charts_1/learning_rate": 0.00018749999999999998, "losses_1/policy_loss": 0.0011842912063002586, "losses_1/entropy": 3.967301845550537, "losses_1/old_approx_kl": 0.00297585129737854, "losses_1/approx_kl": 9.940564632415771e-05, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": 0.0030242204666137695, "global_step": 112, "_wandb": {"runtime": 824}}