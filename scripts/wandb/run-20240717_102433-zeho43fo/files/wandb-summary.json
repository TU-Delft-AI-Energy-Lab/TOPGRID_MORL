{"train/reward_0": 0.0, "train/reward_1": 0.598641574382782, "train/reward_2": -0.1, "train/grid2opsteps": 1, "_timestamp": 1721204759.5072553, "_runtime": 85.99844121932983, "_step": 16, "eval/reward_0": 0.3504960317460318, "eval/reward_1": 272.8533048858245, "eval/reward_2": -37.26999999999898, "losses_1/value_loss": 213299.796875, "charts_1/learning_rate": 0.00015, "losses_1/policy_loss": 0.0001398921012878418, "losses_1/entropy": 3.970262050628662, "losses_1/old_approx_kl": -0.00018960237503051758, "losses_1/approx_kl": 2.9802322387695312e-08, "losses_1/clipfrac": 0.0, "losses_1/explained_variance": -0.0007116794586181641, "global_step": 16, "_wandb": {"runtime": 85}}